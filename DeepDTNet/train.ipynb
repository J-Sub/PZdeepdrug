{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset\n",
    "새로운 data에 대해서는 아래 class 새로 구현해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinationDataset(Dataset):\n",
    "    def __init__(self, database='DCDB', dimension='3D', neg_ratio=1, transform=None):\n",
    "        '''\n",
    "        Args:\n",
    "            database (str): database 이름 (DCDB, C_DCDB)\n",
    "            dimension (str): drug embedding dimension (3D, 12D)\n",
    "            neg_ratio (int): negative sample의 비율. 1이면 positive sample과 동일, 2이면 2배, 3이면 3배\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        '''\n",
    "        if (database != 'DCDB') & (database != 'C_DCDB'):\n",
    "            raise ValueError('database must be DCDB or C_DCDB')\n",
    "        if (dimension != '3D') & (dimension != '12D'):\n",
    "            raise ValueError('dimension must be 3D or 12D')\n",
    "        if neg_ratio < 1:\n",
    "            raise ValueError('neg_ratio must be greater than 1')\n",
    "        self.dimension = dimension\n",
    "        self.database = database\n",
    "        self.transform = transform\n",
    "        self.neg_ratio = neg_ratio\n",
    "        self.data_path = Path('data/processed')/f'{database}_{dimension}_neg{neg_ratio}.pt'\n",
    "        if self.data_path.exists():\n",
    "            self.data = torch.load(self.data_path)\n",
    "        else:\n",
    "            self._process()\n",
    "            self.data = torch.load(self.data_path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def _process(self):\n",
    "        print('Processing dataset...')\n",
    "        dataset_list = self._create_dataset()\n",
    "        print(f'Saving dataset...{self.data_path}')\n",
    "        torch.save(dataset_list, self.data_path)\n",
    "\n",
    "    def _create_dataset(self):\n",
    "        # Get drug embedding\n",
    "        _embedding_filelist = os.listdir('data/embedding')\n",
    "        embedding = [Path('data/embedding')/x for x in _embedding_filelist if self.dimension in x]\n",
    "        # Concatenate drug embeddings of each network\n",
    "        drug_embedding = pd.concat([pd.read_csv(x) for x in embedding], axis=1).to_numpy()\n",
    "        print(f'Shape of concatenated drug embedding: {drug_embedding.shape}')\n",
    "        # Create drug dictionary\n",
    "        _drug_list = pd.read_csv('data/raw/drug_dict.txt', sep=':', header=None)[0].tolist()\n",
    "        drug_id2idx = {_drug_list[i]:i for i in range(len(_drug_list))}\n",
    "        drug_idx2id = {i:_drug_list[i] for i in range(len(_drug_list))}\n",
    "        # Prepare positive labels\n",
    "        pos_df = pd.read_csv(f'data/labels/{self.database}_deepdtnet.tsv', sep='\\t')\n",
    "        \n",
    "        dataset_list = []\n",
    "        # positive samples\n",
    "        for i in range(len(pos_df)):\n",
    "            drug1_id = pos_df.iloc[i, 0]\n",
    "            drug2_id = pos_df.iloc[i, 1]\n",
    "            comb_embedding = np.concatenate([drug_embedding[drug_id2idx[drug1_id]], drug_embedding[drug_id2idx[drug2_id]]])\n",
    "            dataset_list.append([torch.tensor(comb_embedding, dtype=torch.float), torch.tensor(1, dtype=torch.long)])\n",
    "        # negative samples\n",
    "        n_drug = drug_embedding.shape[0]\n",
    "        while len(dataset_list) < len(pos_df) * (1 + self.neg_ratio):\n",
    "            drug1_id = drug_idx2id[np.random.randint(0, n_drug)]\n",
    "            drug2_id = drug_idx2id[np.random.randint(0, n_drug)]\n",
    "            if drug1_id == drug2_id:\n",
    "                continue\n",
    "            if ((pos_df['drug_1'] == drug1_id) & (pos_df['drug_2'] == drug2_id)).any():\n",
    "                continue\n",
    "            if ((pos_df['drug_1'] == drug2_id) & (pos_df['drug_2'] == drug1_id)).any():\n",
    "                continue\n",
    "            comb_embedding = np.concatenate([drug_embedding[drug_id2idx[drug1_id]], drug_embedding[drug_id2idx[drug2_id]]])\n",
    "            dataset_list.append([torch.tensor(comb_embedding, dtype=torch.float), torch.tensor(0, dtype=torch.long)])\n",
    "        return dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset...\n",
      "Shape of concatenated drug embedding: (732, 72)\n",
      "Saving dataset...data/processed/C_DCDB_12D.pt\n",
      "3224\n"
     ]
    }
   ],
   "source": [
    "dataset = CombinationDataset(database='C_DCDB', dimension='12D', neg_ratio=1)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - valid_size\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, comb_type='cat', dropout=0.1):\n",
    "        super(CombNet, self).__init__()\n",
    "        self.input_dim = input_dim # dimension of concatenated drug embeddings\n",
    "        if (comb_type != 'cat') & (comb_type != 'sum') & (comb_type != 'diff') & (comb_type != 'sumdiff'):\n",
    "            raise ValueError('comb_type must be cat, sum, diff or sumdiff')\n",
    "        self.comb_type = comb_type\n",
    "        self.lt = nn.Sequential(\n",
    "            nn.Linear(input_dim // 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        dual_dim = hidden_dim if (comb_type == 'sum' or comb_type == 'diff') else hidden_dim * 2\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(dual_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            # nn.Linear(hidden_dim, hidden_dim),\n",
    "            # nn.BatchNorm1d(hidden_dim),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim) # for BCEWithLogitsLoss\n",
    "        )\n",
    "    def forward(self, data):\n",
    "        drug1, drug2 = data[:, :self.input_dim//2], data[:, self.input_dim//2:] # drug1과 drug2를 분리\n",
    "        drug1, drug2 = self.lt(drug1), self.lt(drug2)\n",
    "        if self.comb_type == 'cat': # [(drug1), (drug2)]로 concat\n",
    "            comb = torch.cat([drug1, drug2], dim=1) # (batch_size, hidden_dim * 2)\n",
    "        elif self.comb_type == 'sum': # [(drug1) + (drug2)]\n",
    "            comb = drug1 + drug2 # (batch_size, hidden_dim)\n",
    "        elif self.comb_type == 'diff': # [(drug1) - (drug2)]\n",
    "            comb = torch.abs(drug1 - drug2) # (batch_size, hidden_dim)\n",
    "        elif self.comb_type == 'sumdiff': # [(drug1) + (drug2), (drug1) - (drug2)]로 concat\n",
    "            comb = torch.cat([drug1 + drug2, torch.abs(drug1 - drug2)], dim=1) # (batch_size, hidden_dim * 2)\n",
    "        return self.fc(comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, metric='accuracy'):\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data).view(-1) # z\n",
    "        # print(output)\n",
    "        loss = criterion(output, target) # z, y\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if metric == 'accuracy':\n",
    "            pred = torch.sigmoid(output).round()\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        elif metric == 'auc':\n",
    "            pass\n",
    "    return train_loss / (batch_idx + 1), correct / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device, loader, criterion, metric='accuracy', checkpoint=None):\n",
    "    # evaluate\n",
    "    if checkpoint is not None:\n",
    "        model.load_state_dict(torch.load(checkpoint))\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(loader):\n",
    "            data, target = data.to(device), target.float().to(device)\n",
    "            output = model(data).view(-1)\n",
    "            eval_loss += criterion(output, target).item()\n",
    "            if metric == 'accuracy':\n",
    "                pred = torch.sigmoid(output).round()\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            elif metric == 'auc':\n",
    "                pass\n",
    "    return eval_loss / (batch_idx + 1), correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = dataset[0][0].shape[0]\n",
    "hidden_dim = input_dim\n",
    "output_dim = 1\n",
    "model = CombNet(input_dim, hidden_dim, output_dim, comb_type='cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "LR = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 0.6801 | Train Acc: 58.36 Val. Loss: 0.6759 | Val. Acc: 54.97%\n",
      "Epoch 002: | Train Loss: 0.6577 | Train Acc: 60.49 Val. Loss: 0.7416 | Val. Acc: 54.04%\n",
      "Epoch 003: | Train Loss: 0.6331 | Train Acc: 64.13 Val. Loss: 0.6847 | Val. Acc: 56.21%\n",
      "Epoch 004: | Train Loss: 0.6252 | Train Acc: 64.75 Val. Loss: 0.6740 | Val. Acc: 58.70%\n",
      "Epoch 005: | Train Loss: 0.6054 | Train Acc: 65.88 Val. Loss: 0.6663 | Val. Acc: 56.52%\n",
      "Epoch 006: | Train Loss: 0.6084 | Train Acc: 66.38 Val. Loss: 0.6821 | Val. Acc: 56.83%\n",
      "Epoch 007: | Train Loss: 0.5862 | Train Acc: 68.05 Val. Loss: 0.6861 | Val. Acc: 61.80%\n",
      "Epoch 008: | Train Loss: 0.5748 | Train Acc: 70.22 Val. Loss: 0.6747 | Val. Acc: 58.39%\n",
      "Epoch 009: | Train Loss: 0.5697 | Train Acc: 70.03 Val. Loss: 0.6483 | Val. Acc: 61.49%\n",
      "Epoch 010: | Train Loss: 0.5559 | Train Acc: 71.27 Val. Loss: 0.5812 | Val. Acc: 67.39%\n",
      "Epoch 011: | Train Loss: 0.5425 | Train Acc: 73.05 Val. Loss: 0.5565 | Val. Acc: 67.39%\n",
      "Epoch 012: | Train Loss: 0.5483 | Train Acc: 71.19 Val. Loss: 0.5667 | Val. Acc: 69.57%\n",
      "Epoch 013: | Train Loss: 0.5295 | Train Acc: 73.05 Val. Loss: 0.6082 | Val. Acc: 66.46%\n",
      "Epoch 014: | Train Loss: 0.5127 | Train Acc: 74.41 Val. Loss: 0.5612 | Val. Acc: 68.01%\n",
      "Epoch 015: | Train Loss: 0.5158 | Train Acc: 74.37 Val. Loss: 0.5829 | Val. Acc: 64.29%\n",
      "Epoch 016: | Train Loss: 0.5018 | Train Acc: 75.69 Val. Loss: 0.6819 | Val. Acc: 65.84%\n",
      "Epoch 017: | Train Loss: 0.5037 | Train Acc: 75.73 Val. Loss: 0.6401 | Val. Acc: 63.98%\n",
      "Epoch 018: | Train Loss: 0.4976 | Train Acc: 76.15 Val. Loss: 0.6110 | Val. Acc: 69.25%\n",
      "Epoch 019: | Train Loss: 0.4791 | Train Acc: 77.63 Val. Loss: 0.6923 | Val. Acc: 65.53%\n",
      "Epoch 020: | Train Loss: 0.4720 | Train Acc: 77.12 Val. Loss: 0.5864 | Val. Acc: 65.53%\n",
      "Epoch 021: | Train Loss: 0.4742 | Train Acc: 77.67 Val. Loss: 0.5781 | Val. Acc: 69.88%\n",
      "Epoch 022: | Train Loss: 0.4703 | Train Acc: 77.39 Val. Loss: 0.7314 | Val. Acc: 63.66%\n",
      "Epoch 023: | Train Loss: 0.4625 | Train Acc: 78.40 Val. Loss: 0.6228 | Val. Acc: 68.01%\n",
      "Epoch 024: | Train Loss: 0.4442 | Train Acc: 79.64 Val. Loss: 0.6206 | Val. Acc: 66.77%\n",
      "Epoch 025: | Train Loss: 0.4427 | Train Acc: 79.84 Val. Loss: 0.5160 | Val. Acc: 68.94%\n",
      "Epoch 026: | Train Loss: 0.4404 | Train Acc: 79.33 Val. Loss: 0.5940 | Val. Acc: 70.81%\n",
      "Epoch 027: | Train Loss: 0.4372 | Train Acc: 79.22 Val. Loss: 0.4848 | Val. Acc: 75.47%\n",
      "Epoch 028: | Train Loss: 0.4259 | Train Acc: 79.99 Val. Loss: 0.5253 | Val. Acc: 72.05%\n",
      "Epoch 029: | Train Loss: 0.4326 | Train Acc: 79.57 Val. Loss: 0.5181 | Val. Acc: 69.57%\n",
      "Epoch 030: | Train Loss: 0.4250 | Train Acc: 80.88 Val. Loss: 0.5045 | Val. Acc: 73.91%\n",
      "Epoch 031: | Train Loss: 0.4108 | Train Acc: 81.08 Val. Loss: 0.5104 | Val. Acc: 70.50%\n",
      "Epoch 032: | Train Loss: 0.4086 | Train Acc: 82.24 Val. Loss: 0.5348 | Val. Acc: 74.84%\n",
      "Epoch 033: | Train Loss: 0.4070 | Train Acc: 81.97 Val. Loss: 0.5211 | Val. Acc: 72.36%\n",
      "Epoch 034: | Train Loss: 0.3869 | Train Acc: 81.81 Val. Loss: 0.5815 | Val. Acc: 70.81%\n",
      "Epoch 035: | Train Loss: 0.4022 | Train Acc: 81.47 Val. Loss: 0.5636 | Val. Acc: 75.47%\n",
      "Epoch 036: | Train Loss: 0.3959 | Train Acc: 82.40 Val. Loss: 0.5041 | Val. Acc: 73.29%\n",
      "Epoch 037: | Train Loss: 0.3804 | Train Acc: 82.98 Val. Loss: 0.5040 | Val. Acc: 74.22%\n",
      "Epoch 038: | Train Loss: 0.3962 | Train Acc: 82.47 Val. Loss: 0.5186 | Val. Acc: 71.43%\n",
      "Epoch 039: | Train Loss: 0.3875 | Train Acc: 82.44 Val. Loss: 0.5250 | Val. Acc: 72.05%\n",
      "Epoch 040: | Train Loss: 0.3743 | Train Acc: 82.90 Val. Loss: 0.4757 | Val. Acc: 75.78%\n",
      "Epoch 041: | Train Loss: 0.3786 | Train Acc: 83.48 Val. Loss: 0.4909 | Val. Acc: 75.47%\n",
      "Epoch 042: | Train Loss: 0.3700 | Train Acc: 84.53 Val. Loss: 0.5877 | Val. Acc: 73.60%\n",
      "Epoch 043: | Train Loss: 0.3638 | Train Acc: 84.30 Val. Loss: 0.6090 | Val. Acc: 69.88%\n",
      "Epoch 044: | Train Loss: 0.3628 | Train Acc: 83.25 Val. Loss: 0.5592 | Val. Acc: 75.16%\n",
      "Epoch 045: | Train Loss: 0.3649 | Train Acc: 83.60 Val. Loss: 0.6385 | Val. Acc: 73.29%\n",
      "Epoch 046: | Train Loss: 0.3495 | Train Acc: 84.76 Val. Loss: 0.5950 | Val. Acc: 73.29%\n",
      "Epoch 047: | Train Loss: 0.3478 | Train Acc: 84.68 Val. Loss: 0.5152 | Val. Acc: 75.47%\n",
      "Epoch 048: | Train Loss: 0.3452 | Train Acc: 85.38 Val. Loss: 0.5395 | Val. Acc: 74.84%\n",
      "Epoch 049: | Train Loss: 0.3486 | Train Acc: 84.34 Val. Loss: 0.4987 | Val. Acc: 77.64%\n",
      "Epoch 050: | Train Loss: 0.3324 | Train Acc: 85.46 Val. Loss: 0.6012 | Val. Acc: 71.12%\n",
      "Epoch 051: | Train Loss: 0.3435 | Train Acc: 84.65 Val. Loss: 0.5144 | Val. Acc: 75.47%\n",
      "Epoch 052: | Train Loss: 0.3396 | Train Acc: 85.19 Val. Loss: 0.5063 | Val. Acc: 74.84%\n",
      "Epoch 053: | Train Loss: 0.3405 | Train Acc: 85.30 Val. Loss: 0.6160 | Val. Acc: 70.81%\n",
      "Epoch 054: | Train Loss: 0.3179 | Train Acc: 86.74 Val. Loss: 0.5840 | Val. Acc: 76.71%\n",
      "Epoch 055: | Train Loss: 0.3285 | Train Acc: 85.61 Val. Loss: 0.5334 | Val. Acc: 73.29%\n",
      "Epoch 056: | Train Loss: 0.3191 | Train Acc: 87.09 Val. Loss: 0.4768 | Val. Acc: 75.16%\n",
      "Epoch 057: | Train Loss: 0.3195 | Train Acc: 85.42 Val. Loss: 0.5017 | Val. Acc: 73.91%\n",
      "Epoch 058: | Train Loss: 0.3184 | Train Acc: 86.23 Val. Loss: 0.6377 | Val. Acc: 73.60%\n",
      "Epoch 059: | Train Loss: 0.3078 | Train Acc: 87.40 Val. Loss: 0.5476 | Val. Acc: 73.29%\n",
      "Epoch 060: | Train Loss: 0.3048 | Train Acc: 86.97 Val. Loss: 0.5651 | Val. Acc: 72.67%\n",
      "Epoch 061: | Train Loss: 0.2999 | Train Acc: 87.17 Val. Loss: 0.6040 | Val. Acc: 72.98%\n",
      "Epoch 062: | Train Loss: 0.3162 | Train Acc: 86.74 Val. Loss: 0.5903 | Val. Acc: 72.36%\n",
      "Epoch 063: | Train Loss: 0.3119 | Train Acc: 86.23 Val. Loss: 0.5775 | Val. Acc: 71.12%\n",
      "Epoch 064: | Train Loss: 0.3053 | Train Acc: 87.40 Val. Loss: 0.6283 | Val. Acc: 74.53%\n",
      "Epoch 065: | Train Loss: 0.2971 | Train Acc: 86.93 Val. Loss: 0.4491 | Val. Acc: 76.40%\n",
      "Epoch 066: | Train Loss: 0.3056 | Train Acc: 86.93 Val. Loss: 0.5406 | Val. Acc: 72.67%\n",
      "Epoch 067: | Train Loss: 0.3059 | Train Acc: 87.17 Val. Loss: 0.9196 | Val. Acc: 73.60%\n",
      "Epoch 068: | Train Loss: 0.2989 | Train Acc: 87.63 Val. Loss: 0.6012 | Val. Acc: 74.84%\n",
      "Epoch 069: | Train Loss: 0.2962 | Train Acc: 87.67 Val. Loss: 0.5848 | Val. Acc: 76.71%\n",
      "Epoch 070: | Train Loss: 0.2915 | Train Acc: 87.75 Val. Loss: 0.5157 | Val. Acc: 76.09%\n",
      "Epoch 071: | Train Loss: 0.2788 | Train Acc: 88.02 Val. Loss: 0.5310 | Val. Acc: 72.36%\n",
      "Epoch 072: | Train Loss: 0.2818 | Train Acc: 88.21 Val. Loss: 0.4587 | Val. Acc: 75.47%\n",
      "Epoch 073: | Train Loss: 0.2854 | Train Acc: 88.33 Val. Loss: 0.4566 | Val. Acc: 76.71%\n",
      "Epoch 074: | Train Loss: 0.2856 | Train Acc: 87.71 Val. Loss: 0.5605 | Val. Acc: 72.05%\n",
      "Epoch 075: | Train Loss: 0.2744 | Train Acc: 88.17 Val. Loss: 0.5836 | Val. Acc: 75.78%\n",
      "Epoch 076: | Train Loss: 0.2795 | Train Acc: 88.10 Val. Loss: 0.4686 | Val. Acc: 75.16%\n",
      "Epoch 077: | Train Loss: 0.2886 | Train Acc: 86.86 Val. Loss: 0.5932 | Val. Acc: 74.22%\n",
      "Epoch 078: | Train Loss: 0.2654 | Train Acc: 88.60 Val. Loss: 0.5163 | Val. Acc: 75.47%\n",
      "Epoch 079: | Train Loss: 0.2679 | Train Acc: 89.45 Val. Loss: 0.6193 | Val. Acc: 75.16%\n",
      "Epoch 080: | Train Loss: 0.2742 | Train Acc: 87.98 Val. Loss: 0.6485 | Val. Acc: 71.12%\n",
      "Epoch 081: | Train Loss: 0.2661 | Train Acc: 89.26 Val. Loss: 0.5277 | Val. Acc: 74.53%\n",
      "Epoch 082: | Train Loss: 0.2699 | Train Acc: 88.52 Val. Loss: 0.6162 | Val. Acc: 73.60%\n",
      "Epoch 083: | Train Loss: 0.2614 | Train Acc: 89.07 Val. Loss: 0.5749 | Val. Acc: 75.78%\n",
      "Epoch 084: | Train Loss: 0.2683 | Train Acc: 89.22 Val. Loss: 0.5605 | Val. Acc: 72.98%\n",
      "Epoch 085: | Train Loss: 0.2527 | Train Acc: 89.49 Val. Loss: 0.5910 | Val. Acc: 71.43%\n",
      "Epoch 086: | Train Loss: 0.2560 | Train Acc: 89.30 Val. Loss: 0.6101 | Val. Acc: 73.91%\n",
      "Epoch 087: | Train Loss: 0.2642 | Train Acc: 89.30 Val. Loss: 0.5379 | Val. Acc: 76.40%\n",
      "Epoch 088: | Train Loss: 0.2599 | Train Acc: 88.99 Val. Loss: 0.5556 | Val. Acc: 71.43%\n",
      "Epoch 089: | Train Loss: 0.2582 | Train Acc: 89.88 Val. Loss: 0.5212 | Val. Acc: 77.64%\n",
      "Epoch 090: | Train Loss: 0.2501 | Train Acc: 89.88 Val. Loss: 0.5124 | Val. Acc: 75.47%\n",
      "Epoch 091: | Train Loss: 0.2564 | Train Acc: 88.95 Val. Loss: 0.4739 | Val. Acc: 79.50%\n",
      "Epoch 092: | Train Loss: 0.2490 | Train Acc: 90.23 Val. Loss: 0.6442 | Val. Acc: 76.71%\n",
      "Epoch 093: | Train Loss: 0.2453 | Train Acc: 89.53 Val. Loss: 0.5595 | Val. Acc: 75.16%\n",
      "Epoch 094: | Train Loss: 0.2515 | Train Acc: 89.26 Val. Loss: 0.5870 | Val. Acc: 73.91%\n",
      "Epoch 095: | Train Loss: 0.2451 | Train Acc: 89.61 Val. Loss: 0.5948 | Val. Acc: 73.29%\n",
      "Epoch 096: | Train Loss: 0.2425 | Train Acc: 90.00 Val. Loss: 0.5264 | Val. Acc: 74.22%\n",
      "Epoch 097: | Train Loss: 0.2331 | Train Acc: 90.46 Val. Loss: 0.6137 | Val. Acc: 73.91%\n",
      "Epoch 098: | Train Loss: 0.2489 | Train Acc: 89.38 Val. Loss: 0.5892 | Val. Acc: 73.29%\n",
      "Epoch 099: | Train Loss: 0.2464 | Train Acc: 88.79 Val. Loss: 0.6117 | Val. Acc: 73.29%\n",
      "Epoch 100: | Train Loss: 0.2514 | Train Acc: 89.41 Val. Loss: 0.4995 | Val. Acc: 76.40%\n",
      "Epoch 101: | Train Loss: 0.2299 | Train Acc: 90.07 Val. Loss: 0.4886 | Val. Acc: 77.95%\n",
      "Epoch 102: | Train Loss: 0.2389 | Train Acc: 89.49 Val. Loss: 0.8472 | Val. Acc: 72.67%\n",
      "Epoch 103: | Train Loss: 0.2266 | Train Acc: 91.43 Val. Loss: 0.6554 | Val. Acc: 74.53%\n",
      "Epoch 104: | Train Loss: 0.2332 | Train Acc: 89.65 Val. Loss: 0.5174 | Val. Acc: 74.22%\n",
      "Epoch 105: | Train Loss: 0.2473 | Train Acc: 89.57 Val. Loss: 0.6966 | Val. Acc: 73.60%\n",
      "Epoch 106: | Train Loss: 0.2137 | Train Acc: 91.90 Val. Loss: 0.5489 | Val. Acc: 74.84%\n",
      "Epoch 107: | Train Loss: 0.2340 | Train Acc: 90.89 Val. Loss: 0.5288 | Val. Acc: 76.09%\n",
      "Epoch 108: | Train Loss: 0.2184 | Train Acc: 91.20 Val. Loss: 0.4980 | Val. Acc: 77.02%\n",
      "Epoch 109: | Train Loss: 0.2318 | Train Acc: 90.35 Val. Loss: 0.5131 | Val. Acc: 75.47%\n",
      "Epoch 110: | Train Loss: 0.2293 | Train Acc: 90.07 Val. Loss: 0.5963 | Val. Acc: 76.40%\n",
      "Epoch 111: | Train Loss: 0.2376 | Train Acc: 90.19 Val. Loss: 0.5165 | Val. Acc: 76.71%\n",
      "Epoch 112: | Train Loss: 0.2370 | Train Acc: 90.35 Val. Loss: 0.6036 | Val. Acc: 74.53%\n",
      "Epoch 113: | Train Loss: 0.2360 | Train Acc: 90.23 Val. Loss: 0.5800 | Val. Acc: 73.91%\n",
      "Epoch 114: | Train Loss: 0.2392 | Train Acc: 90.31 Val. Loss: 0.8128 | Val. Acc: 74.53%\n",
      "Epoch 115: | Train Loss: 0.2439 | Train Acc: 89.30 Val. Loss: 0.8021 | Val. Acc: 72.67%\n",
      "Epoch 116: | Train Loss: 0.2171 | Train Acc: 90.66 Val. Loss: 0.5442 | Val. Acc: 75.16%\n",
      "Epoch 117: | Train Loss: 0.2118 | Train Acc: 91.04 Val. Loss: 0.6044 | Val. Acc: 76.71%\n",
      "Epoch 118: | Train Loss: 0.2175 | Train Acc: 91.28 Val. Loss: 0.6541 | Val. Acc: 73.60%\n",
      "Epoch 119: | Train Loss: 0.1997 | Train Acc: 91.74 Val. Loss: 0.5276 | Val. Acc: 78.57%\n",
      "Epoch 120: | Train Loss: 0.2343 | Train Acc: 89.92 Val. Loss: 0.5486 | Val. Acc: 75.78%\n",
      "Epoch 121: | Train Loss: 0.2132 | Train Acc: 91.47 Val. Loss: 0.6764 | Val. Acc: 72.05%\n",
      "Epoch 122: | Train Loss: 0.2201 | Train Acc: 91.39 Val. Loss: 0.6990 | Val. Acc: 75.47%\n",
      "Epoch 123: | Train Loss: 0.2104 | Train Acc: 91.47 Val. Loss: 0.5552 | Val. Acc: 74.22%\n",
      "Epoch 124: | Train Loss: 0.2029 | Train Acc: 91.47 Val. Loss: 0.5535 | Val. Acc: 74.84%\n",
      "Epoch 125: | Train Loss: 0.2272 | Train Acc: 90.97 Val. Loss: 0.5765 | Val. Acc: 75.47%\n",
      "Epoch 126: | Train Loss: 0.2194 | Train Acc: 90.93 Val. Loss: 0.4656 | Val. Acc: 80.12%\n",
      "Epoch 127: | Train Loss: 0.2207 | Train Acc: 91.16 Val. Loss: 0.7045 | Val. Acc: 74.22%\n",
      "Epoch 128: | Train Loss: 0.2121 | Train Acc: 91.31 Val. Loss: 0.5363 | Val. Acc: 77.64%\n",
      "Epoch 129: | Train Loss: 0.2300 | Train Acc: 90.11 Val. Loss: 0.5598 | Val. Acc: 74.53%\n",
      "Epoch 130: | Train Loss: 0.2149 | Train Acc: 90.69 Val. Loss: 0.5097 | Val. Acc: 76.71%\n",
      "Epoch 131: | Train Loss: 0.2026 | Train Acc: 91.04 Val. Loss: 0.5257 | Val. Acc: 76.09%\n",
      "Epoch 132: | Train Loss: 0.2001 | Train Acc: 92.40 Val. Loss: 0.6583 | Val. Acc: 76.09%\n",
      "Epoch 133: | Train Loss: 0.2009 | Train Acc: 91.93 Val. Loss: 0.9163 | Val. Acc: 74.22%\n",
      "Epoch 134: | Train Loss: 0.2028 | Train Acc: 91.51 Val. Loss: 0.5648 | Val. Acc: 74.84%\n",
      "Epoch 135: | Train Loss: 0.2049 | Train Acc: 91.16 Val. Loss: 0.6992 | Val. Acc: 72.67%\n",
      "Epoch 136: | Train Loss: 0.1960 | Train Acc: 92.32 Val. Loss: 0.5919 | Val. Acc: 77.64%\n",
      "Epoch 137: | Train Loss: 0.1886 | Train Acc: 92.63 Val. Loss: 0.7951 | Val. Acc: 76.40%\n",
      "Epoch 138: | Train Loss: 0.2094 | Train Acc: 91.86 Val. Loss: 0.6472 | Val. Acc: 74.53%\n",
      "Epoch 139: | Train Loss: 0.2047 | Train Acc: 91.35 Val. Loss: 0.6327 | Val. Acc: 74.84%\n",
      "Epoch 140: | Train Loss: 0.1808 | Train Acc: 93.25 Val. Loss: 0.5616 | Val. Acc: 74.22%\n",
      "Epoch 141: | Train Loss: 0.1898 | Train Acc: 92.44 Val. Loss: 0.5886 | Val. Acc: 74.84%\n",
      "Epoch 142: | Train Loss: 0.1951 | Train Acc: 91.86 Val. Loss: 0.5468 | Val. Acc: 75.78%\n",
      "Epoch 143: | Train Loss: 0.1977 | Train Acc: 92.01 Val. Loss: 0.5100 | Val. Acc: 75.16%\n",
      "Epoch 144: | Train Loss: 0.2012 | Train Acc: 91.74 Val. Loss: 0.5582 | Val. Acc: 74.84%\n",
      "Epoch 145: | Train Loss: 0.2002 | Train Acc: 91.97 Val. Loss: 0.5805 | Val. Acc: 79.50%\n",
      "Epoch 146: | Train Loss: 0.1955 | Train Acc: 91.93 Val. Loss: 0.6080 | Val. Acc: 75.47%\n",
      "Epoch 147: | Train Loss: 0.1789 | Train Acc: 92.94 Val. Loss: 0.5742 | Val. Acc: 74.22%\n",
      "Epoch 148: | Train Loss: 0.1794 | Train Acc: 92.36 Val. Loss: 0.7664 | Val. Acc: 75.78%\n",
      "Epoch 149: | Train Loss: 0.2015 | Train Acc: 92.13 Val. Loss: 0.5324 | Val. Acc: 77.64%\n",
      "Epoch 150: | Train Loss: 0.2076 | Train Acc: 91.51 Val. Loss: 0.8021 | Val. Acc: 74.84%\n",
      "Epoch 151: | Train Loss: 0.2276 | Train Acc: 91.08 Val. Loss: 0.6979 | Val. Acc: 71.74%\n",
      "Epoch 152: | Train Loss: 0.1824 | Train Acc: 92.59 Val. Loss: 0.5644 | Val. Acc: 74.22%\n",
      "Epoch 153: | Train Loss: 0.1782 | Train Acc: 93.37 Val. Loss: 0.5455 | Val. Acc: 75.16%\n",
      "Epoch 154: | Train Loss: 0.1785 | Train Acc: 93.21 Val. Loss: 0.8367 | Val. Acc: 76.71%\n",
      "Epoch 155: | Train Loss: 0.1940 | Train Acc: 92.32 Val. Loss: 0.5680 | Val. Acc: 75.78%\n",
      "Epoch 156: | Train Loss: 0.1891 | Train Acc: 91.97 Val. Loss: 0.7221 | Val. Acc: 76.09%\n",
      "Epoch 157: | Train Loss: 0.1647 | Train Acc: 93.56 Val. Loss: 0.5795 | Val. Acc: 75.78%\n",
      "Epoch 158: | Train Loss: 0.1997 | Train Acc: 92.32 Val. Loss: 0.7392 | Val. Acc: 76.40%\n",
      "Epoch 159: | Train Loss: 0.1878 | Train Acc: 92.01 Val. Loss: 0.5661 | Val. Acc: 75.47%\n",
      "Epoch 160: | Train Loss: 0.2022 | Train Acc: 91.24 Val. Loss: 0.6108 | Val. Acc: 76.40%\n",
      "Epoch 161: | Train Loss: 0.1805 | Train Acc: 92.75 Val. Loss: 0.6149 | Val. Acc: 73.91%\n",
      "Epoch 162: | Train Loss: 0.1771 | Train Acc: 92.25 Val. Loss: 0.5731 | Val. Acc: 74.22%\n",
      "Epoch 163: | Train Loss: 0.1657 | Train Acc: 93.49 Val. Loss: 0.7208 | Val. Acc: 73.91%\n",
      "Epoch 164: | Train Loss: 0.2041 | Train Acc: 91.86 Val. Loss: 0.5691 | Val. Acc: 76.09%\n",
      "Epoch 165: | Train Loss: 0.1696 | Train Acc: 93.02 Val. Loss: 0.5782 | Val. Acc: 77.02%\n",
      "Epoch 166: | Train Loss: 0.1764 | Train Acc: 93.18 Val. Loss: 0.7190 | Val. Acc: 75.16%\n",
      "Epoch 167: | Train Loss: 0.1921 | Train Acc: 92.48 Val. Loss: 0.6673 | Val. Acc: 73.91%\n",
      "Epoch 168: | Train Loss: 0.1882 | Train Acc: 92.36 Val. Loss: 0.6761 | Val. Acc: 74.53%\n",
      "Epoch 169: | Train Loss: 0.1674 | Train Acc: 92.94 Val. Loss: 0.5364 | Val. Acc: 75.47%\n",
      "Epoch 170: | Train Loss: 0.1753 | Train Acc: 92.67 Val. Loss: 0.6490 | Val. Acc: 76.71%\n",
      "Epoch 171: | Train Loss: 0.1749 | Train Acc: 92.90 Val. Loss: 0.5812 | Val. Acc: 75.78%\n",
      "Epoch 172: | Train Loss: 0.1824 | Train Acc: 92.36 Val. Loss: 0.6847 | Val. Acc: 77.95%\n",
      "Epoch 173: | Train Loss: 0.1856 | Train Acc: 92.79 Val. Loss: 0.5971 | Val. Acc: 74.53%\n",
      "Epoch 174: | Train Loss: 0.1809 | Train Acc: 92.79 Val. Loss: 0.6665 | Val. Acc: 73.29%\n",
      "Epoch 175: | Train Loss: 0.1934 | Train Acc: 92.67 Val. Loss: 0.6672 | Val. Acc: 74.84%\n",
      "Epoch 176: | Train Loss: 0.1729 | Train Acc: 93.45 Val. Loss: 0.7183 | Val. Acc: 76.40%\n",
      "Epoch 177: | Train Loss: 0.1675 | Train Acc: 93.60 Val. Loss: 0.5330 | Val. Acc: 77.02%\n",
      "Epoch 178: | Train Loss: 0.1701 | Train Acc: 93.52 Val. Loss: 0.5981 | Val. Acc: 75.78%\n",
      "Epoch 179: | Train Loss: 0.1563 | Train Acc: 93.76 Val. Loss: 0.6138 | Val. Acc: 74.22%\n",
      "Epoch 180: | Train Loss: 0.1774 | Train Acc: 93.10 Val. Loss: 0.7324 | Val. Acc: 73.91%\n",
      "Epoch 181: | Train Loss: 0.1578 | Train Acc: 93.25 Val. Loss: 0.5998 | Val. Acc: 77.02%\n",
      "Epoch 182: | Train Loss: 0.1677 | Train Acc: 93.68 Val. Loss: 0.6273 | Val. Acc: 73.91%\n",
      "Epoch 183: | Train Loss: 0.1692 | Train Acc: 93.41 Val. Loss: 0.7485 | Val. Acc: 74.84%\n",
      "Epoch 184: | Train Loss: 0.1539 | Train Acc: 93.91 Val. Loss: 0.5900 | Val. Acc: 76.09%\n",
      "Epoch 185: | Train Loss: 0.1571 | Train Acc: 94.30 Val. Loss: 0.5903 | Val. Acc: 75.47%\n",
      "Epoch 186: | Train Loss: 0.1757 | Train Acc: 92.90 Val. Loss: 0.8186 | Val. Acc: 75.47%\n",
      "Epoch 187: | Train Loss: 0.1711 | Train Acc: 93.33 Val. Loss: 0.6010 | Val. Acc: 76.09%\n",
      "Epoch 188: | Train Loss: 0.1680 | Train Acc: 93.76 Val. Loss: 0.5556 | Val. Acc: 75.47%\n",
      "Epoch 189: | Train Loss: 0.1592 | Train Acc: 93.99 Val. Loss: 0.6939 | Val. Acc: 74.84%\n",
      "Epoch 190: | Train Loss: 0.1591 | Train Acc: 93.49 Val. Loss: 0.5342 | Val. Acc: 76.40%\n",
      "Epoch 191: | Train Loss: 0.1632 | Train Acc: 93.60 Val. Loss: 0.5715 | Val. Acc: 78.57%\n",
      "Epoch 192: | Train Loss: 0.1472 | Train Acc: 94.46 Val. Loss: 0.5271 | Val. Acc: 80.43%\n",
      "Epoch 193: | Train Loss: 0.1467 | Train Acc: 94.49 Val. Loss: 0.6149 | Val. Acc: 75.47%\n",
      "Epoch 194: | Train Loss: 0.1648 | Train Acc: 92.87 Val. Loss: 0.5384 | Val. Acc: 78.57%\n",
      "Epoch 195: | Train Loss: 0.1392 | Train Acc: 94.46 Val. Loss: 0.6520 | Val. Acc: 75.78%\n",
      "Epoch 196: | Train Loss: 0.1635 | Train Acc: 93.72 Val. Loss: 0.7873 | Val. Acc: 69.88%\n",
      "Epoch 197: | Train Loss: 0.1686 | Train Acc: 93.56 Val. Loss: 0.7583 | Val. Acc: 76.71%\n",
      "Epoch 198: | Train Loss: 0.1756 | Train Acc: 93.25 Val. Loss: 0.5671 | Val. Acc: 76.40%\n",
      "Epoch 199: | Train Loss: 0.1581 | Train Acc: 93.76 Val. Loss: 0.8423 | Val. Acc: 74.84%\n",
      "Epoch 200: | Train Loss: 0.1674 | Train Acc: 93.49 Val. Loss: 0.5727 | Val. Acc: 74.84%\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, device, train_loader, criterion, optimizer)\n",
    "    valid_loss, valid_acc = evaluate(model, device, valid_loader, criterion)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "    print(f'Epoch {epoch+1:03d}: | Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f} Val. Loss: {valid_loss:.4f} | Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5904 | Test Acc: 70.59%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, device, test_loader, criterion, checkpoint='model.pt')\n",
    "print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_list = pd.read_csv('data/raw/drug_dict.txt', sep=':', header=None)[0].tolist() # list of drugs in DrugBank id\n",
    "drug_id2idx = {drug_list[i]:i for i in range(len(drug_list))} # dictionary of drugs - key: id, value: index\n",
    "drug_idx2id = {i:drug_list[i] for i in range(len(drug_list))} # dictionary of drugs - key: index, value: id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "486"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(drug_id2idx['DB01098']) # rosuvastatin\n",
    "print(drug_id2idx['DB01076']) # atorvastatin\n",
    "print(drug_id2idx['DB01039']) # fenofibrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be added..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('aidd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e227c0bd2d5950a59e33e5abbabd7d4ec83cf9ddf6ae0e499e1468ef19a6f5a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
