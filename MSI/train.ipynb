{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from load_data import LoadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import AUROC, Accuracy, Precision, Recall\n",
    "from torchmetrics.classification import BinaryAUROC, BinaryF1Score\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinationDataset(Dataset):\n",
    "    def __init__(self, database='DCDB', neg_ratio=1, duplicate=False, transform=None):\n",
    "        if (database != 'DCDB') & (database != 'C_DCDB'):\n",
    "            raise ValueError('database must be DCDB or C_DCDB')\n",
    "        if neg_ratio < 1:\n",
    "            raise ValueError('neg_ratio must be greater than 1')\n",
    "        self.database = database\n",
    "        self.neg_ratio = neg_ratio\n",
    "        self.transform = transform\n",
    "        self.duplicate = duplicate\n",
    "        self.data_path = Path('data/processed')/f'{database}_neg{neg_ratio}_dup{int(duplicate)}.pt'\n",
    "        if self.data_path.exists():\n",
    "            self.data = torch.load(self.data_path)\n",
    "        else:\n",
    "            self._process()\n",
    "            self.data = torch.load(self.data_path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def _process(self):\n",
    "        print('Processing dataset...')\n",
    "        dataset_list = self._create_dataset()\n",
    "        print(f'Saving dataset...{self.data_path}')\n",
    "        torch.save(dataset_list, self.data_path)\n",
    "    \n",
    "    def _create_dataset(self):\n",
    "        dataloader = LoadData()\n",
    "        # Get embedding\n",
    "        with open('data/embedding/embeddings_node2vec_msi_seed0.pkl', 'rb') as f:\n",
    "            embedding_dict = pickle.load(f)\n",
    "        # drug dictionary\n",
    "        drug_id2name, drug_name2id = dataloader.get_dict(type='drug')\n",
    "        # Prepare positive labels\n",
    "        pos_df = pd.read_csv(f'data/labels/{self.database}_msi.tsv', sep='\\t')\n",
    "\n",
    "        dataset_list = []\n",
    "        # positive samples\n",
    "        for i in range(len(pos_df)):\n",
    "            drug1_id = pos_df.iloc[i, 0]\n",
    "            drug2_id = pos_df.iloc[i, 1]\n",
    "            comb_embedding = np.concatenate([embedding_dict[drug1_id], embedding_dict[drug2_id]])\n",
    "            dataset_list.append([torch.tensor(comb_embedding, dtype=torch.float), torch.tensor(1, dtype=torch.long)])\n",
    "            if self.duplicate:\n",
    "                comb_embedding2 = np.concatenate([embedding_dict[drug2_id], embedding_dict[drug1_id]])\n",
    "                dataset_list.append([torch.tensor(comb_embedding2, dtype=torch.float), torch.tensor(1, dtype=torch.long)])\n",
    "                \n",
    "        # negative samples\n",
    "        count = 0\n",
    "        while count < len(pos_df) * self.neg_ratio:\n",
    "        # while len(dataset_list) < len(pos_df) * (1 + self.neg_ratio):\n",
    "            drug1_id = random.choice(list(drug_id2name.keys()))\n",
    "            drug2_id = random.choice(list(drug_id2name.keys()))\n",
    "            if drug1_id == drug2_id:\n",
    "                continue\n",
    "            if ((pos_df['drug_1'] == drug1_id) & (pos_df['drug_2'] == drug2_id)).any():\n",
    "                continue\n",
    "            if ((pos_df['drug_1'] == drug2_id) & (pos_df['drug_2'] == drug1_id)).any():\n",
    "                continue\n",
    "            comb_embedding = np.concatenate([embedding_dict[drug1_id], embedding_dict[drug2_id]])\n",
    "            dataset_list.append([torch.tensor(comb_embedding, dtype=torch.float), torch.tensor(0, dtype=torch.long)])\n",
    "            if self.duplicate:\n",
    "                comb_embedding2 = np.concatenate([embedding_dict[drug2_id], embedding_dict[drug1_id]])\n",
    "                dataset_list.append([torch.tensor(comb_embedding2, dtype=torch.float), torch.tensor(0, dtype=torch.long)])\n",
    "            count += 1\n",
    "        return dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = CombinationDataset(database='DCDB', neg_ratio=1)\n",
    "# print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset...\n",
      "Saving dataset...data/processed/C_DCDB_neg6_dup0.pt\n",
      "29547\n"
     ]
    }
   ],
   "source": [
    "dataset = CombinationDataset(database='C_DCDB', neg_ratio=6, duplicate=False)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - valid_size\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# need to split data well if duplicate=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, comb_type='cat', dropout=0.1):\n",
    "        super(CombNet, self).__init__()\n",
    "        self.input_dim = input_dim # dimension of concatenated drug embeddings\n",
    "        if (comb_type != 'cat') & (comb_type != 'sum') & (comb_type != 'diff') & (comb_type != 'sumdiff'):\n",
    "            raise ValueError('comb_type must be cat, sum, diff or sumdiff')\n",
    "        self.comb_type = comb_type\n",
    "        self.lt = nn.Sequential(\n",
    "            nn.Linear(input_dim // 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        dual_dim = hidden_dim if (comb_type == 'sum' or comb_type == 'diff') else hidden_dim * 2\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(dual_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            # nn.Linear(hidden_dim, hidden_dim),\n",
    "            # nn.BatchNorm1d(hidden_dim),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim) # for BCEWithLogitsLoss\n",
    "        )\n",
    "    def forward(self, data):\n",
    "        drug1, drug2 = data[:, :self.input_dim//2], data[:, self.input_dim//2:] # drug1과 drug2를 분리\n",
    "        drug1, drug2 = self.lt(drug1), self.lt(drug2)\n",
    "        if self.comb_type == 'cat': # [(drug1), (drug2)]로 concat\n",
    "            comb = torch.cat([drug1, drug2], dim=1) # (batch_size, hidden_dim * 2)\n",
    "        elif self.comb_type == 'sum': # [(drug1) + (drug2)]\n",
    "            comb = drug1 + drug2 # (batch_size, hidden_dim)\n",
    "        elif self.comb_type == 'diff': # [(drug1) - (drug2)]\n",
    "            comb = torch.abs(drug1 - drug2) # (batch_size, hidden_dim)\n",
    "        elif self.comb_type == 'sumdiff': # [(drug1) + (drug2), (drug1) - (drug2)]로 concat\n",
    "            comb = torch.cat([drug1 + drug2, torch.abs(drug1 - drug2)], dim=1) # (batch_size, hidden_dim * 2)\n",
    "        return self.fc(comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, metric_list=[accuracy_score]):\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    target_list = []\n",
    "    pred_list = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data).view(-1) # z\n",
    "        # print(output)\n",
    "        loss = criterion(output, target) # z, y\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        pred_list.append(torch.sigmoid(output).detach().cpu().numpy())\n",
    "        target_list.append(target.long().detach().cpu().numpy())\n",
    "    \n",
    "    # metric\n",
    "    scores = []\n",
    "    for metric in metric_list:\n",
    "        if (metric == roc_auc_score) or (metric == average_precision_score):\n",
    "            scores.append(metric(np.concatenate(target_list), np.concatenate(pred_list)))\n",
    "        else: # accuracy_score, f1_score, precision_score, recall_score\n",
    "            scores.append(metric(np.concatenate(target_list), np.concatenate(pred_list).round()))\n",
    "    \n",
    "    return train_loss / (batch_idx + 1), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device, loader, criterion, metric_list=[accuracy_score], checkpoint=None):\n",
    "    # evaluate\n",
    "    if checkpoint is not None:\n",
    "        model.load_state_dict(torch.load(checkpoint))\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "\n",
    "    target_list = []\n",
    "    pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(loader):\n",
    "            data, target = data.to(device), target.float().to(device)\n",
    "            output = model(data).view(-1)\n",
    "            eval_loss += criterion(output, target).item()\n",
    "            pred_list.append(torch.sigmoid(output).detach().cpu().numpy())\n",
    "            target_list.append(target.long().detach().cpu().numpy())\n",
    "\n",
    "    scores = []\n",
    "    for metric in metric_list:\n",
    "        if (metric == roc_auc_score) or (metric == average_precision_score):\n",
    "            scores.append(metric(np.concatenate(target_list), np.concatenate(pred_list)))\n",
    "        else: # accuracy_score, f1_score, precision_score, recall_score\n",
    "            scores.append(metric(np.concatenate(target_list), np.concatenate(pred_list).round()))\n",
    "    return eval_loss / (batch_idx + 1), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = dataset[0][0].shape[0]\n",
    "hidden_dim = input_dim\n",
    "output_dim = 1\n",
    "model = CombNet(input_dim, hidden_dim, output_dim, comb_type='cat')\n",
    "# model = CombNet(input_dim, hidden_dim, output_dim, comb_type='sum')\n",
    "# model = CombNet(input_dim, hidden_dim, output_dim, comb_type='sumdiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "LR = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 0.3302 | Train Acc: 87.53% | Train AUROC: 0.79 | Train F1: 0.3792 | Train AUPRC: 0.48 || Val. Loss: 0.2815 | Val. Acc: 89.30% | Val. AUROC: 0.86 | Val. F1: 0.5486 | Val. AUPRC: 0.61\n",
      "Epoch 002: | Train Loss: 0.2685 | Train Acc: 89.60% | Train AUROC: 0.88 | Train F1: 0.5410 | Train AUPRC: 0.64 || Val. Loss: 0.2546 | Val. Acc: 90.69% | Val. AUROC: 0.89 | Val. F1: 0.5902 | Val. AUPRC: 0.67\n",
      "Epoch 003: | Train Loss: 0.2441 | Train Acc: 90.34% | Train AUROC: 0.90 | Train F1: 0.5905 | Train AUPRC: 0.69 || Val. Loss: 0.2421 | Val. Acc: 90.52% | Val. AUROC: 0.90 | Val. F1: 0.5796 | Val. AUPRC: 0.68\n",
      "Epoch 004: | Train Loss: 0.2255 | Train Acc: 91.22% | Train AUROC: 0.92 | Train F1: 0.6361 | Train AUPRC: 0.73 || Val. Loss: 0.2422 | Val. Acc: 90.42% | Val. AUROC: 0.91 | Val. F1: 0.6291 | Val. AUPRC: 0.70\n",
      "Epoch 005: | Train Loss: 0.2097 | Train Acc: 91.76% | Train AUROC: 0.93 | Train F1: 0.6674 | Train AUPRC: 0.76 || Val. Loss: 0.2346 | Val. Acc: 91.23% | Val. AUROC: 0.91 | Val. F1: 0.6505 | Val. AUPRC: 0.70\n",
      "Epoch 006: | Train Loss: 0.1956 | Train Acc: 92.10% | Train AUROC: 0.94 | Train F1: 0.6871 | Train AUPRC: 0.78 || Val. Loss: 0.2311 | Val. Acc: 91.30% | Val. AUROC: 0.91 | Val. F1: 0.6513 | Val. AUPRC: 0.70\n",
      "Epoch 007: | Train Loss: 0.1834 | Train Acc: 92.74% | Train AUROC: 0.95 | Train F1: 0.7153 | Train AUPRC: 0.81 || Val. Loss: 0.2352 | Val. Acc: 91.23% | Val. AUROC: 0.91 | Val. F1: 0.6649 | Val. AUPRC: 0.71\n",
      "Epoch 008: | Train Loss: 0.1688 | Train Acc: 93.30% | Train AUROC: 0.96 | Train F1: 0.7438 | Train AUPRC: 0.84 || Val. Loss: 0.2381 | Val. Acc: 91.13% | Val. AUROC: 0.91 | Val. F1: 0.6757 | Val. AUPRC: 0.71\n",
      "Epoch 009: | Train Loss: 0.1583 | Train Acc: 93.54% | Train AUROC: 0.96 | Train F1: 0.7548 | Train AUPRC: 0.85 || Val. Loss: 0.2414 | Val. Acc: 91.33% | Val. AUROC: 0.91 | Val. F1: 0.6693 | Val. AUPRC: 0.71\n",
      "Epoch 010: | Train Loss: 0.1471 | Train Acc: 94.24% | Train AUROC: 0.97 | Train F1: 0.7827 | Train AUPRC: 0.87 || Val. Loss: 0.2532 | Val. Acc: 91.50% | Val. AUROC: 0.91 | Val. F1: 0.6835 | Val. AUPRC: 0.70\n",
      "Epoch 011: | Train Loss: 0.1349 | Train Acc: 94.79% | Train AUROC: 0.97 | Train F1: 0.8067 | Train AUPRC: 0.89 || Val. Loss: 0.2473 | Val. Acc: 91.10% | Val. AUROC: 0.92 | Val. F1: 0.6757 | Val. AUPRC: 0.72\n",
      "Epoch 012: | Train Loss: 0.1255 | Train Acc: 95.05% | Train AUROC: 0.98 | Train F1: 0.8175 | Train AUPRC: 0.91 || Val. Loss: 0.2459 | Val. Acc: 91.16% | Val. AUROC: 0.92 | Val. F1: 0.6758 | Val. AUPRC: 0.71\n",
      "Epoch 013: | Train Loss: 0.1209 | Train Acc: 95.29% | Train AUROC: 0.98 | Train F1: 0.8266 | Train AUPRC: 0.91 || Val. Loss: 0.2647 | Val. Acc: 90.86% | Val. AUROC: 0.91 | Val. F1: 0.6770 | Val. AUPRC: 0.71\n",
      "Epoch 014: | Train Loss: 0.1104 | Train Acc: 95.80% | Train AUROC: 0.98 | Train F1: 0.8465 | Train AUPRC: 0.92 || Val. Loss: 0.2580 | Val. Acc: 90.56% | Val. AUROC: 0.92 | Val. F1: 0.6610 | Val. AUPRC: 0.72\n",
      "Epoch 015: | Train Loss: 0.1056 | Train Acc: 95.97% | Train AUROC: 0.98 | Train F1: 0.8527 | Train AUPRC: 0.93 || Val. Loss: 0.2682 | Val. Acc: 91.06% | Val. AUROC: 0.92 | Val. F1: 0.6865 | Val. AUPRC: 0.72\n",
      "Epoch 016: | Train Loss: 0.0966 | Train Acc: 96.26% | Train AUROC: 0.99 | Train F1: 0.8650 | Train AUPRC: 0.94 || Val. Loss: 0.2782 | Val. Acc: 90.96% | Val. AUROC: 0.92 | Val. F1: 0.6810 | Val. AUPRC: 0.72\n",
      "Epoch 017: | Train Loss: 0.0929 | Train Acc: 96.46% | Train AUROC: 0.99 | Train F1: 0.8720 | Train AUPRC: 0.95 || Val. Loss: 0.2881 | Val. Acc: 90.69% | Val. AUROC: 0.91 | Val. F1: 0.6617 | Val. AUPRC: 0.71\n",
      "Epoch 018: | Train Loss: 0.0865 | Train Acc: 96.57% | Train AUROC: 0.99 | Train F1: 0.8766 | Train AUPRC: 0.95 || Val. Loss: 0.2990 | Val. Acc: 90.35% | Val. AUROC: 0.91 | Val. F1: 0.6460 | Val. AUPRC: 0.70\n",
      "Epoch 019: | Train Loss: 0.0886 | Train Acc: 96.52% | Train AUROC: 0.99 | Train F1: 0.8751 | Train AUPRC: 0.95 || Val. Loss: 0.2854 | Val. Acc: 91.40% | Val. AUROC: 0.91 | Val. F1: 0.6825 | Val. AUPRC: 0.72\n",
      "Epoch 020: | Train Loss: 0.0782 | Train Acc: 97.10% | Train AUROC: 0.99 | Train F1: 0.8966 | Train AUPRC: 0.96 || Val. Loss: 0.3011 | Val. Acc: 90.83% | Val. AUROC: 0.91 | Val. F1: 0.6642 | Val. AUPRC: 0.69\n",
      "Epoch 021: | Train Loss: 0.0789 | Train Acc: 96.95% | Train AUROC: 0.99 | Train F1: 0.8904 | Train AUPRC: 0.96 || Val. Loss: 0.3047 | Val. Acc: 91.06% | Val. AUROC: 0.91 | Val. F1: 0.6675 | Val. AUPRC: 0.71\n",
      "Epoch 022: | Train Loss: 0.0734 | Train Acc: 97.27% | Train AUROC: 0.99 | Train F1: 0.9025 | Train AUPRC: 0.96 || Val. Loss: 0.3040 | Val. Acc: 91.13% | Val. AUROC: 0.91 | Val. F1: 0.6725 | Val. AUPRC: 0.71\n",
      "Epoch 023: | Train Loss: 0.0700 | Train Acc: 97.41% | Train AUROC: 0.99 | Train F1: 0.9076 | Train AUPRC: 0.97 || Val. Loss: 0.3039 | Val. Acc: 91.06% | Val. AUROC: 0.92 | Val. F1: 0.6675 | Val. AUPRC: 0.71\n",
      "Epoch 024: | Train Loss: 0.0717 | Train Acc: 97.25% | Train AUROC: 0.99 | Train F1: 0.9015 | Train AUPRC: 0.97 || Val. Loss: 0.3142 | Val. Acc: 90.69% | Val. AUROC: 0.91 | Val. F1: 0.6532 | Val. AUPRC: 0.71\n",
      "Epoch 025: | Train Loss: 0.0677 | Train Acc: 97.42% | Train AUROC: 0.99 | Train F1: 0.9083 | Train AUPRC: 0.97 || Val. Loss: 0.3129 | Val. Acc: 91.30% | Val. AUROC: 0.92 | Val. F1: 0.6734 | Val. AUPRC: 0.72\n",
      "Epoch 026: | Train Loss: 0.0629 | Train Acc: 97.78% | Train AUROC: 0.99 | Train F1: 0.9209 | Train AUPRC: 0.97 || Val. Loss: 0.3176 | Val. Acc: 90.72% | Val. AUROC: 0.92 | Val. F1: 0.6683 | Val. AUPRC: 0.70\n",
      "Epoch 027: | Train Loss: 0.0602 | Train Acc: 97.79% | Train AUROC: 1.00 | Train F1: 0.9217 | Train AUPRC: 0.98 || Val. Loss: 0.3270 | Val. Acc: 91.37% | Val. AUROC: 0.92 | Val. F1: 0.6658 | Val. AUPRC: 0.71\n",
      "Epoch 028: | Train Loss: 0.0609 | Train Acc: 97.80% | Train AUROC: 0.99 | Train F1: 0.9220 | Train AUPRC: 0.97 || Val. Loss: 0.3317 | Val. Acc: 91.23% | Val. AUROC: 0.92 | Val. F1: 0.6701 | Val. AUPRC: 0.71\n",
      "Epoch 029: | Train Loss: 0.0560 | Train Acc: 97.93% | Train AUROC: 1.00 | Train F1: 0.9267 | Train AUPRC: 0.98 || Val. Loss: 0.3360 | Val. Acc: 91.37% | Val. AUROC: 0.91 | Val. F1: 0.6631 | Val. AUPRC: 0.72\n",
      "Epoch 030: | Train Loss: 0.0555 | Train Acc: 97.88% | Train AUROC: 1.00 | Train F1: 0.9250 | Train AUPRC: 0.98 || Val. Loss: 0.3416 | Val. Acc: 91.27% | Val. AUROC: 0.92 | Val. F1: 0.6684 | Val. AUPRC: 0.70\n",
      "Epoch 031: | Train Loss: 0.0553 | Train Acc: 97.99% | Train AUROC: 1.00 | Train F1: 0.9287 | Train AUPRC: 0.98 || Val. Loss: 0.3376 | Val. Acc: 91.10% | Val. AUROC: 0.91 | Val. F1: 0.6725 | Val. AUPRC: 0.72\n",
      "Epoch 032: | Train Loss: 0.0552 | Train Acc: 98.09% | Train AUROC: 1.00 | Train F1: 0.9324 | Train AUPRC: 0.98 || Val. Loss: 0.3426 | Val. Acc: 90.76% | Val. AUROC: 0.91 | Val. F1: 0.6486 | Val. AUPRC: 0.68\n",
      "Epoch 033: | Train Loss: 0.0537 | Train Acc: 97.95% | Train AUROC: 1.00 | Train F1: 0.9276 | Train AUPRC: 0.98 || Val. Loss: 0.3551 | Val. Acc: 91.47% | Val. AUROC: 0.91 | Val. F1: 0.6810 | Val. AUPRC: 0.70\n",
      "Epoch 034: | Train Loss: 0.0536 | Train Acc: 97.99% | Train AUROC: 1.00 | Train F1: 0.9289 | Train AUPRC: 0.98 || Val. Loss: 0.3606 | Val. Acc: 91.27% | Val. AUROC: 0.91 | Val. F1: 0.6799 | Val. AUPRC: 0.68\n",
      "Epoch 035: | Train Loss: 0.0503 | Train Acc: 98.27% | Train AUROC: 1.00 | Train F1: 0.9387 | Train AUPRC: 0.98 || Val. Loss: 0.3544 | Val. Acc: 91.06% | Val. AUROC: 0.91 | Val. F1: 0.6607 | Val. AUPRC: 0.71\n",
      "Epoch 036: | Train Loss: 0.0470 | Train Acc: 98.27% | Train AUROC: 1.00 | Train F1: 0.9389 | Train AUPRC: 0.98 || Val. Loss: 0.3632 | Val. Acc: 91.06% | Val. AUROC: 0.91 | Val. F1: 0.6650 | Val. AUPRC: 0.71\n",
      "Epoch 037: | Train Loss: 0.0488 | Train Acc: 98.17% | Train AUROC: 1.00 | Train F1: 0.9355 | Train AUPRC: 0.98 || Val. Loss: 0.3538 | Val. Acc: 91.06% | Val. AUROC: 0.91 | Val. F1: 0.6749 | Val. AUPRC: 0.71\n",
      "Epoch 038: | Train Loss: 0.0493 | Train Acc: 98.25% | Train AUROC: 1.00 | Train F1: 0.9382 | Train AUPRC: 0.98 || Val. Loss: 0.3374 | Val. Acc: 91.10% | Val. AUROC: 0.92 | Val. F1: 0.6733 | Val. AUPRC: 0.72\n",
      "Epoch 039: | Train Loss: 0.0445 | Train Acc: 98.40% | Train AUROC: 1.00 | Train F1: 0.9432 | Train AUPRC: 0.99 || Val. Loss: 0.3644 | Val. Acc: 90.76% | Val. AUROC: 0.92 | Val. F1: 0.6617 | Val. AUPRC: 0.70\n",
      "Epoch 040: | Train Loss: 0.0478 | Train Acc: 98.24% | Train AUROC: 1.00 | Train F1: 0.9379 | Train AUPRC: 0.98 || Val. Loss: 0.3685 | Val. Acc: 90.66% | Val. AUROC: 0.92 | Val. F1: 0.6497 | Val. AUPRC: 0.70\n",
      "Epoch 041: | Train Loss: 0.0491 | Train Acc: 98.15% | Train AUROC: 1.00 | Train F1: 0.9346 | Train AUPRC: 0.98 || Val. Loss: 0.3782 | Val. Acc: 90.59% | Val. AUROC: 0.91 | Val. F1: 0.6568 | Val. AUPRC: 0.69\n",
      "Epoch 042: | Train Loss: 0.0467 | Train Acc: 98.31% | Train AUROC: 1.00 | Train F1: 0.9405 | Train AUPRC: 0.98 || Val. Loss: 0.3640 | Val. Acc: 91.06% | Val. AUROC: 0.92 | Val. F1: 0.6607 | Val. AUPRC: 0.71\n",
      "Epoch 043: | Train Loss: 0.0471 | Train Acc: 98.36% | Train AUROC: 1.00 | Train F1: 0.9417 | Train AUPRC: 0.98 || Val. Loss: 0.3764 | Val. Acc: 90.08% | Val. AUROC: 0.91 | Val. F1: 0.6369 | Val. AUPRC: 0.68\n",
      "Epoch 044: | Train Loss: 0.0430 | Train Acc: 98.45% | Train AUROC: 1.00 | Train F1: 0.9453 | Train AUPRC: 0.99 || Val. Loss: 0.3914 | Val. Acc: 90.08% | Val. AUROC: 0.91 | Val. F1: 0.6396 | Val. AUPRC: 0.68\n",
      "Epoch 045: | Train Loss: 0.0443 | Train Acc: 98.42% | Train AUROC: 1.00 | Train F1: 0.9444 | Train AUPRC: 0.99 || Val. Loss: 0.4096 | Val. Acc: 91.40% | Val. AUROC: 0.91 | Val. F1: 0.6631 | Val. AUPRC: 0.69\n",
      "Epoch 046: | Train Loss: 0.0414 | Train Acc: 98.53% | Train AUROC: 1.00 | Train F1: 0.9482 | Train AUPRC: 0.99 || Val. Loss: 0.3977 | Val. Acc: 89.98% | Val. AUROC: 0.91 | Val. F1: 0.6468 | Val. AUPRC: 0.69\n",
      "Epoch 047: | Train Loss: 0.0441 | Train Acc: 98.46% | Train AUROC: 1.00 | Train F1: 0.9457 | Train AUPRC: 0.99 || Val. Loss: 0.4024 | Val. Acc: 90.83% | Val. AUROC: 0.91 | Val. F1: 0.6591 | Val. AUPRC: 0.70\n",
      "Epoch 048: | Train Loss: 0.0416 | Train Acc: 98.50% | Train AUROC: 1.00 | Train F1: 0.9469 | Train AUPRC: 0.99 || Val. Loss: 0.3809 | Val. Acc: 91.40% | Val. AUROC: 0.92 | Val. F1: 0.6760 | Val. AUPRC: 0.72\n",
      "Epoch 049: | Train Loss: 0.0435 | Train Acc: 98.45% | Train AUROC: 1.00 | Train F1: 0.9454 | Train AUPRC: 0.99 || Val. Loss: 0.3809 | Val. Acc: 91.10% | Val. AUROC: 0.92 | Val. F1: 0.6820 | Val. AUPRC: 0.72\n",
      "Epoch 050: | Train Loss: 0.0412 | Train Acc: 98.53% | Train AUROC: 1.00 | Train F1: 0.9481 | Train AUPRC: 0.99 || Val. Loss: 0.3914 | Val. Acc: 90.59% | Val. AUROC: 0.91 | Val. F1: 0.6525 | Val. AUPRC: 0.71\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "    # train_loss, train_acc = train(model, device, train_loader, criterion, optimizer)\n",
    "    # valid_loss, valid_acc = evaluate(model, device, valid_loader, criterion)\n",
    "    # train_loss, train_scores = train(model, device, train_loader, criterion, optimizer, metric_list=[accuracy_score, roc_auc_score])\n",
    "    # valid_loss, valid_scores = evaluate(model, device, valid_loader, criterion, metric_list=[accuracy_score, roc_auc_score])\n",
    "    train_loss, train_scores = train(model, device, train_loader, criterion, optimizer, metric_list=[accuracy_score, roc_auc_score, f1_score, average_precision_score])\n",
    "    valid_loss, valid_scores = evaluate(model, device, valid_loader, criterion, metric_list=[accuracy_score, roc_auc_score, f1_score, average_precision_score])\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "    # print(f'Epoch {epoch+1:03d}: | Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}% | Train AUROC: {train_auroc:.2f} || Val. Loss: {valid_loss:.4f} | Val. Acc: {valid_acc*100:.2f}% | Val. AUROC: {valid_auroc:.2f}')\n",
    "    # print(f'Epoch {epoch+1:03d}: | Train Loss: {train_loss:.4f} | Train scores: {train_scores} || Val. Loss: {valid_loss:.4f} | Val. scores: {valid_scores}')\n",
    "    # print(f'Epoch {epoch+1:03d}: | Train Loss: {train_loss:.4f} | Train Acc: {train_scores[0]*100:.2f}% | Train AUROC: {train_scores[1]:.2f} || Val. Loss: {valid_loss:.4f} | Val. Acc: {valid_scores[0]*100:.2f}% | Val. AUROC: {valid_scores[1]:.2f}')\n",
    "    # print(f'Epoch {epoch+1:03d}: | Train Loss: {train_loss:.4f} | Train Acc: {train_scores[0]*100:.2f}% | Train F1: {train_scores[1]:.4f} | Train AUROC: {train_scores[2]:.2f} || Val. Loss: {valid_loss:.4f} | Val. Acc: {valid_scores[0]*100:.2f}% | Val. F1: {valid_scores[1]:.4f} | Val. AUROC: {valid_scores[2]:.2f}')\n",
    "    print(f'Epoch {epoch+1:03d}: | Train Loss: {train_loss:.4f} | Train Acc: {train_scores[0]*100:.2f}% | Train AUROC: {train_scores[1]:.2f} | Train F1: {train_scores[2]:.4f} | Train AUPRC: {train_scores[3]:.2f} || Val. Loss: {valid_loss:.4f} | Val. Acc: {valid_scores[0]*100:.2f}% | Val. AUROC: {valid_scores[1]:.2f} | Val. F1: {valid_scores[2]:.4f} | Val. AUPRC: {valid_scores[3]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2352 | Test Acc: 90.46% | Test AUROC: 0.92 | Test F1: 0.6627 | Test AUPRC: 0.74\n"
     ]
    }
   ],
   "source": [
    "# test_loss, test_acc = evaluate(model, device, test_loader, criterion, checkpoint='model.pt')\n",
    "# test_loss, test_acc, test_auroc = evaluate(model, device, test_loader, criterion, metric='both', checkpoint='model.pt')\n",
    "# test_loss, test_scores = evaluate(model, device, test_loader, criterion, metric_list=[accuracy_score, roc_auc_score], checkpoint='checkpoint.pt')\n",
    "test_loss, test_scores = evaluate(model, device, test_loader, criterion, metric_list=[accuracy_score, roc_auc_score, f1_score, average_precision_score], checkpoint='checkpoint.pt')\n",
    "# print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}% | Test AUROC: {test_auroc:.2f}')\n",
    "# print(f'Test Loss: {test_loss:.4f} | Test scores: {test_scores}')\n",
    "# print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_scores[0]*100:.2f}% | Test AUROC: {test_scores[1]:.2f}')\n",
    "# print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_scores[0]*100:.2f}% | Test F1: {test_scores[1]:.4f} | Test AUROC: {test_scores[2]:.2f}')\n",
    "print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_scores[0]*100:.2f}% | Test AUROC: {test_scores[1]:.2f} | Test F1: {test_scores[2]:.4f} | Test AUPRC: {test_scores[3]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Positive : Negative ratio|F1 score|AUPRC score|\n",
    "|-----|-----|-----|\n",
    "|1:1|0.79|0.87| \n",
    "|1:2|0.74|0.83|\n",
    "|1:3|0.68|0.77|\n",
    "|1:6|0.66|0.74|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class imbalance 문제를 좀 개선하는 방법이 있으면 그걸 novelty로 추가?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ayn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4698125b34f9b3056e7b596654ef06bea4fe54a8b707ab96252cf01711dc60f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
