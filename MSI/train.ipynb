{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from load_data import LoadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import AUROC, Accuracy, Precision, Recall\n",
    "from torchmetrics.classification import BinaryAUROC, BinaryF1Score\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinationDataset(Dataset):\n",
    "    def __init__(self, database='DCDB', neg_ratio=1, duplicate=False, transform=None):\n",
    "        if (database != 'DCDB') & (database != 'C_DCDB'):\n",
    "            raise ValueError('database must be DCDB or C_DCDB')\n",
    "        if neg_ratio < 1:\n",
    "            raise ValueError('neg_ratio must be greater than 1')\n",
    "        self.database = database\n",
    "        self.neg_ratio = neg_ratio\n",
    "        self.transform = transform\n",
    "        self.duplicate = duplicate\n",
    "        self.data_path = Path('data/processed')/f'{database}_neg{neg_ratio}_dup{int(duplicate)}.pt'\n",
    "        if self.data_path.exists():\n",
    "            self.data = torch.load(self.data_path)\n",
    "        else:\n",
    "            self._process()\n",
    "            self.data = torch.load(self.data_path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def _process(self):\n",
    "        print('Processing dataset...')\n",
    "        dataset_list = self._create_dataset()\n",
    "        print(f'Saving dataset...{self.data_path}')\n",
    "        torch.save(dataset_list, self.data_path)\n",
    "    \n",
    "    def _create_dataset(self):\n",
    "        dataloader = LoadData()\n",
    "        # Get embedding\n",
    "        with open('data/embedding/embeddings_node2vec_msi_seed0.pkl', 'rb') as f:\n",
    "            embedding_dict = pickle.load(f)\n",
    "        # drug dictionary\n",
    "        drug_id2name, drug_name2id = dataloader.get_dict(type='drug')\n",
    "        # Prepare positive labels\n",
    "        pos_df = pd.read_csv(f'data/labels/{self.database}_msi.tsv', sep='\\t')\n",
    "\n",
    "        dataset_list = []\n",
    "        # positive samples\n",
    "        for i in range(len(pos_df)):\n",
    "            drug1_id = pos_df.iloc[i, 0]\n",
    "            drug2_id = pos_df.iloc[i, 1]\n",
    "            comb_embedding = np.concatenate([embedding_dict[drug1_id], embedding_dict[drug2_id]])\n",
    "            dataset_list.append([torch.tensor(comb_embedding, dtype=torch.float), torch.tensor(1, dtype=torch.long)])\n",
    "            if self.duplicate:\n",
    "                comb_embedding2 = np.concatenate([embedding_dict[drug2_id], embedding_dict[drug1_id]])\n",
    "                dataset_list.append([torch.tensor(comb_embedding2, dtype=torch.float), torch.tensor(1, dtype=torch.long)])\n",
    "                \n",
    "        # negative samples\n",
    "        count = 0\n",
    "        while count < len(pos_df) * self.neg_ratio:\n",
    "        # while len(dataset_list) < len(pos_df) * (1 + self.neg_ratio):\n",
    "            drug1_id = random.choice(list(drug_id2name.keys()))\n",
    "            drug2_id = random.choice(list(drug_id2name.keys()))\n",
    "            if drug1_id == drug2_id:\n",
    "                continue\n",
    "            if ((pos_df['drug_1'] == drug1_id) & (pos_df['drug_2'] == drug2_id)).any():\n",
    "                continue\n",
    "            if ((pos_df['drug_1'] == drug2_id) & (pos_df['drug_2'] == drug1_id)).any():\n",
    "                continue\n",
    "            comb_embedding = np.concatenate([embedding_dict[drug1_id], embedding_dict[drug2_id]])\n",
    "            dataset_list.append([torch.tensor(comb_embedding, dtype=torch.float), torch.tensor(0, dtype=torch.long)])\n",
    "            if self.duplicate:\n",
    "                comb_embedding2 = np.concatenate([embedding_dict[drug2_id], embedding_dict[drug1_id]])\n",
    "                dataset_list.append([torch.tensor(comb_embedding2, dtype=torch.float), torch.tensor(0, dtype=torch.long)])\n",
    "            count += 1\n",
    "        return dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = CombinationDataset(database='DCDB', neg_ratio=1)\n",
    "# print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset...\n",
      "Saving dataset...data/processed/C_DCDB_neg1_dup0.pt\n",
      "8442\n"
     ]
    }
   ],
   "source": [
    "dataset = CombinationDataset(database='C_DCDB', neg_ratio=1, duplicate=False)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - valid_size\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# need to split data well if duplicate=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, comb_type='cat', dropout=0.1):\n",
    "        super(CombNet, self).__init__()\n",
    "        self.input_dim = input_dim # dimension of concatenated drug embeddings\n",
    "        if (comb_type != 'cat') & (comb_type != 'sum') & (comb_type != 'diff') & (comb_type != 'sumdiff'):\n",
    "            raise ValueError('comb_type must be cat, sum, diff or sumdiff')\n",
    "        self.comb_type = comb_type\n",
    "        self.lt = nn.Sequential(\n",
    "            nn.Linear(input_dim // 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        dual_dim = hidden_dim if (comb_type == 'sum' or comb_type == 'diff') else hidden_dim * 2\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(dual_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            # nn.Linear(hidden_dim, hidden_dim),\n",
    "            # nn.BatchNorm1d(hidden_dim),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim) # for BCEWithLogitsLoss\n",
    "        )\n",
    "    def forward(self, data):\n",
    "        drug1, drug2 = data[:, :self.input_dim//2], data[:, self.input_dim//2:] # drug1과 drug2를 분리\n",
    "        drug1, drug2 = self.lt(drug1), self.lt(drug2)\n",
    "        if self.comb_type == 'cat': # [(drug1), (drug2)]로 concat\n",
    "            comb = torch.cat([drug1, drug2], dim=1) # (batch_size, hidden_dim * 2)\n",
    "        elif self.comb_type == 'sum': # [(drug1) + (drug2)]\n",
    "            comb = drug1 + drug2 # (batch_size, hidden_dim)\n",
    "        elif self.comb_type == 'diff': # [(drug1) - (drug2)]\n",
    "            comb = torch.abs(drug1 - drug2) # (batch_size, hidden_dim)\n",
    "        elif self.comb_type == 'sumdiff': # [(drug1) + (drug2), (drug1) - (drug2)]로 concat\n",
    "            comb = torch.cat([drug1 + drug2, torch.abs(drug1 - drug2)], dim=1) # (batch_size, hidden_dim * 2)\n",
    "        return self.fc(comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, metric_list=[accuracy_score]):\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    target_list = []\n",
    "    pred_list = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data).view(-1) # z\n",
    "        # print(output)\n",
    "        loss = criterion(output, target) # z, y\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        pred_list.append(torch.sigmoid(output).detach().cpu().numpy())\n",
    "        target_list.append(target.long().detach().cpu().numpy())\n",
    "    \n",
    "    # metric\n",
    "    scores = []\n",
    "    for metric in metric_list:\n",
    "        if metric == roc_auc_score:\n",
    "            scores.append(metric(np.concatenate(target_list), np.concatenate(pred_list)))\n",
    "        else: # accuracy_score, f1_score, precision_score, recall_score\n",
    "            scores.append(metric(np.concatenate(target_list), np.concatenate(pred_list).round()))\n",
    "    \n",
    "    return train_loss / (batch_idx + 1), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device, loader, criterion, metric_list=[accuracy_score], checkpoint=None):\n",
    "    # evaluate\n",
    "    if checkpoint is not None:\n",
    "        model.load_state_dict(torch.load(checkpoint))\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "\n",
    "    target_list = []\n",
    "    pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(loader):\n",
    "            data, target = data.to(device), target.float().to(device)\n",
    "            output = model(data).view(-1)\n",
    "            eval_loss += criterion(output, target).item()\n",
    "            pred_list.append(torch.sigmoid(output).detach().cpu().numpy())\n",
    "            target_list.append(target.long().detach().cpu().numpy())\n",
    "\n",
    "    scores = []\n",
    "    for metric in metric_list:\n",
    "        if metric == roc_auc_score:\n",
    "            scores.append(metric(np.concatenate(target_list), np.concatenate(pred_list)))\n",
    "        else: # accuracy_score, f1_score, precision_score, recall_score\n",
    "            scores.append(metric(np.concatenate(target_list), np.concatenate(pred_list).round()))\n",
    "    return eval_loss / (batch_idx + 1), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = dataset[0][0].shape[0]\n",
    "hidden_dim = input_dim\n",
    "output_dim = 1\n",
    "model = CombNet(input_dim, hidden_dim, output_dim, comb_type='cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "LR = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 0.5788 | Train Acc: 68.96% | Train AUROC: 0.76 || Val. Loss: 0.5067 | Val. Acc: 75.00% | Val. AUROC: 0.83\n",
      "Epoch 002: | Train Loss: 0.4546 | Train Acc: 79.31% | Train AUROC: 0.87 || Val. Loss: 0.4676 | Val. Acc: 76.66% | Val. AUROC: 0.86\n",
      "Epoch 003: | Train Loss: 0.4056 | Train Acc: 81.64% | Train AUROC: 0.90 || Val. Loss: 0.4751 | Val. Acc: 77.13% | Val. AUROC: 0.87\n",
      "Epoch 004: | Train Loss: 0.3584 | Train Acc: 84.51% | Train AUROC: 0.92 || Val. Loss: 0.4783 | Val. Acc: 77.49% | Val. AUROC: 0.88\n",
      "Epoch 005: | Train Loss: 0.3193 | Train Acc: 86.72% | Train AUROC: 0.94 || Val. Loss: 0.4678 | Val. Acc: 79.03% | Val. AUROC: 0.87\n",
      "Epoch 006: | Train Loss: 0.3041 | Train Acc: 87.04% | Train AUROC: 0.94 || Val. Loss: 0.4564 | Val. Acc: 79.38% | Val. AUROC: 0.88\n",
      "Epoch 007: | Train Loss: 0.2647 | Train Acc: 89.29% | Train AUROC: 0.96 || Val. Loss: 0.4903 | Val. Acc: 79.03% | Val. AUROC: 0.88\n",
      "Epoch 008: | Train Loss: 0.2388 | Train Acc: 90.46% | Train AUROC: 0.97 || Val. Loss: 0.4867 | Val. Acc: 80.81% | Val. AUROC: 0.89\n",
      "Epoch 009: | Train Loss: 0.2052 | Train Acc: 91.90% | Train AUROC: 0.98 || Val. Loss: 0.4852 | Val. Acc: 81.75% | Val. AUROC: 0.89\n",
      "Epoch 010: | Train Loss: 0.1900 | Train Acc: 92.45% | Train AUROC: 0.98 || Val. Loss: 0.5728 | Val. Acc: 77.61% | Val. AUROC: 0.88\n",
      "Epoch 011: | Train Loss: 0.1824 | Train Acc: 92.55% | Train AUROC: 0.98 || Val. Loss: 0.5298 | Val. Acc: 80.09% | Val. AUROC: 0.88\n",
      "Epoch 012: | Train Loss: 0.1631 | Train Acc: 93.63% | Train AUROC: 0.98 || Val. Loss: 0.5400 | Val. Acc: 79.98% | Val. AUROC: 0.88\n",
      "Epoch 013: | Train Loss: 0.1504 | Train Acc: 94.52% | Train AUROC: 0.99 || Val. Loss: 0.6561 | Val. Acc: 77.37% | Val. AUROC: 0.88\n",
      "Epoch 014: | Train Loss: 0.1417 | Train Acc: 94.61% | Train AUROC: 0.99 || Val. Loss: 0.5557 | Val. Acc: 80.57% | Val. AUROC: 0.88\n",
      "Epoch 015: | Train Loss: 0.1243 | Train Acc: 95.59% | Train AUROC: 0.99 || Val. Loss: 0.6452 | Val. Acc: 78.67% | Val. AUROC: 0.88\n",
      "Epoch 016: | Train Loss: 0.1173 | Train Acc: 95.91% | Train AUROC: 0.99 || Val. Loss: 0.5923 | Val. Acc: 80.21% | Val. AUROC: 0.88\n",
      "Epoch 017: | Train Loss: 0.1119 | Train Acc: 95.81% | Train AUROC: 0.99 || Val. Loss: 0.5990 | Val. Acc: 80.69% | Val. AUROC: 0.89\n",
      "Epoch 018: | Train Loss: 0.1116 | Train Acc: 95.99% | Train AUROC: 0.99 || Val. Loss: 0.6379 | Val. Acc: 79.86% | Val. AUROC: 0.88\n",
      "Epoch 019: | Train Loss: 0.1049 | Train Acc: 96.36% | Train AUROC: 0.99 || Val. Loss: 0.6618 | Val. Acc: 78.08% | Val. AUROC: 0.88\n",
      "Epoch 020: | Train Loss: 0.0919 | Train Acc: 96.74% | Train AUROC: 1.00 || Val. Loss: 0.6775 | Val. Acc: 80.57% | Val. AUROC: 0.88\n",
      "Epoch 021: | Train Loss: 0.0940 | Train Acc: 96.73% | Train AUROC: 0.99 || Val. Loss: 0.6599 | Val. Acc: 81.40% | Val. AUROC: 0.88\n",
      "Epoch 022: | Train Loss: 0.0841 | Train Acc: 97.04% | Train AUROC: 1.00 || Val. Loss: 0.7095 | Val. Acc: 79.38% | Val. AUROC: 0.88\n",
      "Epoch 023: | Train Loss: 0.0854 | Train Acc: 96.74% | Train AUROC: 1.00 || Val. Loss: 0.7106 | Val. Acc: 78.79% | Val. AUROC: 0.88\n",
      "Epoch 024: | Train Loss: 0.0709 | Train Acc: 97.57% | Train AUROC: 1.00 || Val. Loss: 0.6754 | Val. Acc: 79.74% | Val. AUROC: 0.89\n",
      "Epoch 025: | Train Loss: 0.0721 | Train Acc: 97.42% | Train AUROC: 1.00 || Val. Loss: 0.7465 | Val. Acc: 78.08% | Val. AUROC: 0.88\n",
      "Epoch 026: | Train Loss: 0.0683 | Train Acc: 97.69% | Train AUROC: 1.00 || Val. Loss: 0.7282 | Val. Acc: 78.55% | Val. AUROC: 0.89\n",
      "Epoch 027: | Train Loss: 0.0717 | Train Acc: 97.41% | Train AUROC: 1.00 || Val. Loss: 0.7147 | Val. Acc: 80.57% | Val. AUROC: 0.89\n",
      "Epoch 028: | Train Loss: 0.0738 | Train Acc: 97.44% | Train AUROC: 1.00 || Val. Loss: 0.7479 | Val. Acc: 79.27% | Val. AUROC: 0.88\n",
      "Epoch 029: | Train Loss: 0.0687 | Train Acc: 97.56% | Train AUROC: 1.00 || Val. Loss: 0.7307 | Val. Acc: 80.57% | Val. AUROC: 0.88\n",
      "Epoch 030: | Train Loss: 0.0658 | Train Acc: 97.67% | Train AUROC: 1.00 || Val. Loss: 0.7699 | Val. Acc: 81.04% | Val. AUROC: 0.88\n",
      "Epoch 031: | Train Loss: 0.0604 | Train Acc: 97.85% | Train AUROC: 1.00 || Val. Loss: 0.7008 | Val. Acc: 79.74% | Val. AUROC: 0.88\n",
      "Epoch 032: | Train Loss: 0.0614 | Train Acc: 97.93% | Train AUROC: 1.00 || Val. Loss: 0.7779 | Val. Acc: 80.69% | Val. AUROC: 0.88\n",
      "Epoch 033: | Train Loss: 0.0576 | Train Acc: 98.02% | Train AUROC: 1.00 || Val. Loss: 0.8075 | Val. Acc: 79.38% | Val. AUROC: 0.88\n",
      "Epoch 034: | Train Loss: 0.0638 | Train Acc: 97.66% | Train AUROC: 1.00 || Val. Loss: 0.7519 | Val. Acc: 79.27% | Val. AUROC: 0.89\n",
      "Epoch 035: | Train Loss: 0.0647 | Train Acc: 97.90% | Train AUROC: 1.00 || Val. Loss: 0.7944 | Val. Acc: 79.62% | Val. AUROC: 0.88\n",
      "Epoch 036: | Train Loss: 0.0644 | Train Acc: 97.60% | Train AUROC: 1.00 || Val. Loss: 0.7227 | Val. Acc: 79.86% | Val. AUROC: 0.88\n",
      "Epoch 037: | Train Loss: 0.0651 | Train Acc: 97.63% | Train AUROC: 1.00 || Val. Loss: 0.7993 | Val. Acc: 79.50% | Val. AUROC: 0.87\n",
      "Epoch 038: | Train Loss: 0.0543 | Train Acc: 98.18% | Train AUROC: 1.00 || Val. Loss: 0.7877 | Val. Acc: 78.79% | Val. AUROC: 0.88\n",
      "Epoch 039: | Train Loss: 0.0532 | Train Acc: 98.02% | Train AUROC: 1.00 || Val. Loss: 0.8065 | Val. Acc: 80.45% | Val. AUROC: 0.88\n",
      "Epoch 040: | Train Loss: 0.0504 | Train Acc: 98.22% | Train AUROC: 1.00 || Val. Loss: 0.8075 | Val. Acc: 80.57% | Val. AUROC: 0.88\n",
      "Epoch 041: | Train Loss: 0.0480 | Train Acc: 98.37% | Train AUROC: 1.00 || Val. Loss: 0.8042 | Val. Acc: 80.45% | Val. AUROC: 0.88\n",
      "Epoch 042: | Train Loss: 0.0604 | Train Acc: 97.87% | Train AUROC: 1.00 || Val. Loss: 0.7790 | Val. Acc: 80.81% | Val. AUROC: 0.89\n",
      "Epoch 043: | Train Loss: 0.0461 | Train Acc: 98.31% | Train AUROC: 1.00 || Val. Loss: 0.7736 | Val. Acc: 81.75% | Val. AUROC: 0.89\n",
      "Epoch 044: | Train Loss: 0.0488 | Train Acc: 98.24% | Train AUROC: 1.00 || Val. Loss: 0.7828 | Val. Acc: 79.86% | Val. AUROC: 0.88\n",
      "Epoch 045: | Train Loss: 0.0403 | Train Acc: 98.67% | Train AUROC: 1.00 || Val. Loss: 0.7725 | Val. Acc: 79.98% | Val. AUROC: 0.89\n",
      "Epoch 046: | Train Loss: 0.0448 | Train Acc: 98.52% | Train AUROC: 1.00 || Val. Loss: 0.7981 | Val. Acc: 81.99% | Val. AUROC: 0.89\n",
      "Epoch 047: | Train Loss: 0.0527 | Train Acc: 98.21% | Train AUROC: 1.00 || Val. Loss: 0.8027 | Val. Acc: 81.28% | Val. AUROC: 0.89\n",
      "Epoch 048: | Train Loss: 0.0437 | Train Acc: 98.50% | Train AUROC: 1.00 || Val. Loss: 0.8625 | Val. Acc: 80.69% | Val. AUROC: 0.89\n",
      "Epoch 049: | Train Loss: 0.0414 | Train Acc: 98.61% | Train AUROC: 1.00 || Val. Loss: 0.8196 | Val. Acc: 79.86% | Val. AUROC: 0.89\n",
      "Epoch 050: | Train Loss: 0.0442 | Train Acc: 98.47% | Train AUROC: 1.00 || Val. Loss: 0.7871 | Val. Acc: 80.69% | Val. AUROC: 0.89\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "    # train_loss, train_acc = train(model, device, train_loader, criterion, optimizer)\n",
    "    # valid_loss, valid_acc = evaluate(model, device, valid_loader, criterion)\n",
    "    train_loss, train_scores = train(model, device, train_loader, criterion, optimizer, metric_list=[accuracy_score, roc_auc_score])\n",
    "    valid_loss, valid_scores = evaluate(model, device, valid_loader, criterion, metric_list=[accuracy_score, roc_auc_score])\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "    #print(f'Epoch {epoch+1:03d}: | Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}% | Train AUROC: {train_auroc:.2f} || Val. Loss: {valid_loss:.4f} | Val. Acc: {valid_acc*100:.2f}% | Val. AUROC: {valid_auroc:.2f}')\n",
    "    # print(f'Epoch {epoch+1:03d}: | Train Loss: {train_loss:.4f} | Train scores: {train_scores} || Val. Loss: {valid_loss:.4f} | Val. scores: {valid_scores}')\n",
    "    print(f'Epoch {epoch+1:03d}: | Train Loss: {train_loss:.4f} | Train Acc: {train_scores[0]*100:.2f}% | Train AUROC: {train_scores[1]:.2f} || Val. Loss: {valid_loss:.4f} | Val. Acc: {valid_scores[0]*100:.2f}% | Val. AUROC: {valid_scores[1]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4742 | Test Acc: 77.87% | Test AUROC: 0.87\n"
     ]
    }
   ],
   "source": [
    "# test_loss, test_acc = evaluate(model, device, test_loader, criterion, checkpoint='model.pt')\n",
    "# test_loss, test_acc, test_auroc = evaluate(model, device, test_loader, criterion, metric='both', checkpoint='model.pt')\n",
    "test_loss, test_scores = evaluate(model, device, test_loader, criterion, metric_list=[accuracy_score, roc_auc_score], checkpoint='checkpoint.pt')\n",
    "# print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}% | Test AUROC: {test_auroc:.2f}')\n",
    "# print(f'Test Loss: {test_loss:.4f} | Test scores: {test_scores}')\n",
    "print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_scores[0]*100:.2f}% | Test AUROC: {test_scores[1]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ayn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4698125b34f9b3056e7b596654ef06bea4fe54a8b707ab96252cf01711dc60f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
