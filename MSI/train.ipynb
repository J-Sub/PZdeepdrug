{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from load_data import LoadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinationDataset(Dataset):\n",
    "    def __init__(self, database='DCDB', neg_ratio=1, transform=None):\n",
    "        if (database != 'DCDB') & (database != 'C_DCDB'):\n",
    "            raise ValueError('database must be DCDB or C_DCDB')\n",
    "        if neg_ratio < 1:\n",
    "            raise ValueError('neg_ratio must be greater than 1')\n",
    "        self.database = database\n",
    "        self.neg_ratio = neg_ratio\n",
    "        self.transform = transform\n",
    "        self.data_path = Path('data/processed')/f'{database}_neg{neg_ratio}.pt'\n",
    "        if self.data_path.exists():\n",
    "            self.data = torch.load(self.data_path)\n",
    "        else:\n",
    "            self._process()\n",
    "            self.data = torch.load(self.data_path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def _process(self):\n",
    "        print('Processing dataset...')\n",
    "        dataset_list = self._create_dataset()\n",
    "        print(f'Saving dataset...{self.data_path}')\n",
    "        torch.save(dataset_list, self.data_path)\n",
    "    \n",
    "    def _create_dataset(self):\n",
    "        dataloader = LoadData()\n",
    "        # Get embedding\n",
    "        with open('data/embedding/embeddings_node2vec_msi_seed0.pkl', 'rb') as f:\n",
    "            embedding_dict = pickle.load(f)\n",
    "        # drug dictionary\n",
    "        drug_id2name, drug_name2id = dataloader.get_dict(type='drug')\n",
    "        # Prepare positive labels\n",
    "        pos_df = pd.read_csv(f'data/labels/{self.database}_msi.tsv', sep='\\t')\n",
    "\n",
    "        dataset_list = []\n",
    "        # positive samples\n",
    "        for i in range(len(pos_df)):\n",
    "            drug1_id = pos_df.iloc[i, 0]\n",
    "            drug2_id = pos_df.iloc[i, 1]\n",
    "            comb_embedding = np.concatenate([embedding_dict[drug1_id], embedding_dict[drug2_id]])\n",
    "            dataset_list.append([torch.tensor(comb_embedding, dtype=torch.float), torch.tensor(1, dtype=torch.long)])\n",
    "        # negative samples\n",
    "        n_drug = len(drug_id2name)\n",
    "        while len(dataset_list) < len(pos_df) * (1 + self.neg_ratio):\n",
    "            drug1_id = random.choice(list(drug_id2name.keys()))\n",
    "            drug2_id = random.choice(list(drug_id2name.keys()))\n",
    "            if drug1_id == drug2_id:\n",
    "                continue\n",
    "            if ((pos_df['drug_1'] == drug1_id) & (pos_df['drug_2'] == drug2_id)).any():\n",
    "                continue\n",
    "            if ((pos_df['drug_1'] == drug2_id) & (pos_df['drug_2'] == drug1_id)).any():\n",
    "                continue\n",
    "            comb_embedding = np.concatenate([embedding_dict[drug1_id], embedding_dict[drug2_id]])\n",
    "            dataset_list.append([torch.tensor(comb_embedding, dtype=torch.float), torch.tensor(0, dtype=torch.long)])\n",
    "        return dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = CombinationDataset(database='DCDB', neg_ratio=1)\n",
    "# print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8442\n"
     ]
    }
   ],
   "source": [
    "dataset = CombinationDataset(database='C_DCDB', neg_ratio=1)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - valid_size\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, comb_type='cat', dropout=0.1):\n",
    "        super(CombNet, self).__init__()\n",
    "        self.input_dim = input_dim # dimension of concatenated drug embeddings\n",
    "        if (comb_type != 'cat') & (comb_type != 'sum') & (comb_type != 'diff') & (comb_type != 'sumdiff'):\n",
    "            raise ValueError('comb_type must be cat, sum, diff or sumdiff')\n",
    "        self.comb_type = comb_type\n",
    "        self.lt = nn.Sequential(\n",
    "            nn.Linear(input_dim // 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        dual_dim = hidden_dim if (comb_type == 'sum' or comb_type == 'diff') else hidden_dim * 2\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(dual_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            # nn.Linear(hidden_dim, hidden_dim),\n",
    "            # nn.BatchNorm1d(hidden_dim),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim) # for BCEWithLogitsLoss\n",
    "        )\n",
    "    def forward(self, data):\n",
    "        drug1, drug2 = data[:, :self.input_dim//2], data[:, self.input_dim//2:] # drug1과 drug2를 분리\n",
    "        drug1, drug2 = self.lt(drug1), self.lt(drug2)\n",
    "        if self.comb_type == 'cat': # [(drug1), (drug2)]로 concat\n",
    "            comb = torch.cat([drug1, drug2], dim=1) # (batch_size, hidden_dim * 2)\n",
    "        elif self.comb_type == 'sum': # [(drug1) + (drug2)]\n",
    "            comb = drug1 + drug2 # (batch_size, hidden_dim)\n",
    "        elif self.comb_type == 'diff': # [(drug1) - (drug2)]\n",
    "            comb = torch.abs(drug1 - drug2) # (batch_size, hidden_dim)\n",
    "        elif self.comb_type == 'sumdiff': # [(drug1) + (drug2), (drug1) - (drug2)]로 concat\n",
    "            comb = torch.cat([drug1 + drug2, torch.abs(drug1 - drug2)], dim=1) # (batch_size, hidden_dim * 2)\n",
    "        return self.fc(comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, metric='accuracy'):\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data).view(-1) # z\n",
    "        # print(output)\n",
    "        loss = criterion(output, target) # z, y\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if metric == 'accuracy':\n",
    "            pred = torch.sigmoid(output).round()\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        elif metric == 'auc':\n",
    "            pass\n",
    "    return train_loss / (batch_idx + 1), correct / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device, loader, criterion, metric='accuracy', checkpoint=None):\n",
    "    # evaluate\n",
    "    if checkpoint is not None:\n",
    "        model.load_state_dict(torch.load(checkpoint))\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(loader):\n",
    "            data, target = data.to(device), target.float().to(device)\n",
    "            output = model(data).view(-1)\n",
    "            eval_loss += criterion(output, target).item()\n",
    "            if metric == 'accuracy':\n",
    "                pred = torch.sigmoid(output).round()\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            elif metric == 'auc':\n",
    "                pass\n",
    "    return eval_loss / (batch_idx + 1), correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = dataset[0][0].shape[0]\n",
    "hidden_dim = input_dim\n",
    "output_dim = 1\n",
    "model = CombNet(input_dim, hidden_dim, output_dim, comb_type='cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "LR = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 0.5848 | Train Acc: 69.02 Val. Loss: 0.5315 | Val. Acc: 73.58%\n",
      "Epoch 002: | Train Loss: 0.4578 | Train Acc: 78.37 Val. Loss: 0.4970 | Val. Acc: 76.07%\n",
      "Epoch 003: | Train Loss: 0.4012 | Train Acc: 82.32 Val. Loss: 0.5028 | Val. Acc: 74.76%\n",
      "Epoch 004: | Train Loss: 0.3547 | Train Acc: 84.61 Val. Loss: 0.4409 | Val. Acc: 80.33%\n",
      "Epoch 005: | Train Loss: 0.3227 | Train Acc: 85.98 Val. Loss: 0.4586 | Val. Acc: 79.50%\n",
      "Epoch 006: | Train Loss: 0.3025 | Train Acc: 87.47 Val. Loss: 0.5233 | Val. Acc: 76.18%\n",
      "Epoch 007: | Train Loss: 0.2659 | Train Acc: 88.86 Val. Loss: 0.5289 | Val. Acc: 76.78%\n",
      "Epoch 008: | Train Loss: 0.2418 | Train Acc: 90.11 Val. Loss: 0.5206 | Val. Acc: 77.37%\n",
      "Epoch 009: | Train Loss: 0.2119 | Train Acc: 91.68 Val. Loss: 0.5008 | Val. Acc: 78.32%\n",
      "Epoch 010: | Train Loss: 0.1975 | Train Acc: 92.11 Val. Loss: 0.5505 | Val. Acc: 78.79%\n",
      "Epoch 011: | Train Loss: 0.1831 | Train Acc: 92.89 Val. Loss: 0.6089 | Val. Acc: 76.66%\n",
      "Epoch 012: | Train Loss: 0.1633 | Train Acc: 93.75 Val. Loss: 0.6142 | Val. Acc: 78.08%\n",
      "Epoch 013: | Train Loss: 0.1496 | Train Acc: 94.39 Val. Loss: 0.5887 | Val. Acc: 78.32%\n",
      "Epoch 014: | Train Loss: 0.1337 | Train Acc: 94.89 Val. Loss: 0.5688 | Val. Acc: 80.21%\n",
      "Epoch 015: | Train Loss: 0.1277 | Train Acc: 95.38 Val. Loss: 0.6395 | Val. Acc: 78.91%\n",
      "Epoch 016: | Train Loss: 0.1154 | Train Acc: 95.85 Val. Loss: 0.5876 | Val. Acc: 79.62%\n",
      "Epoch 017: | Train Loss: 0.1115 | Train Acc: 95.82 Val. Loss: 0.6389 | Val. Acc: 78.32%\n",
      "Epoch 018: | Train Loss: 0.1103 | Train Acc: 95.85 Val. Loss: 0.5941 | Val. Acc: 80.21%\n",
      "Epoch 019: | Train Loss: 0.0940 | Train Acc: 96.40 Val. Loss: 0.6172 | Val. Acc: 79.50%\n",
      "Epoch 020: | Train Loss: 0.0828 | Train Acc: 97.19 Val. Loss: 0.6221 | Val. Acc: 79.98%\n",
      "Epoch 021: | Train Loss: 0.0912 | Train Acc: 96.67 Val. Loss: 0.6172 | Val. Acc: 80.45%\n",
      "Epoch 022: | Train Loss: 0.0836 | Train Acc: 97.02 Val. Loss: 0.7569 | Val. Acc: 77.01%\n",
      "Epoch 023: | Train Loss: 0.0898 | Train Acc: 96.68 Val. Loss: 0.6725 | Val. Acc: 79.38%\n",
      "Epoch 024: | Train Loss: 0.0761 | Train Acc: 97.25 Val. Loss: 0.6615 | Val. Acc: 79.50%\n",
      "Epoch 025: | Train Loss: 0.0723 | Train Acc: 97.51 Val. Loss: 0.7467 | Val. Acc: 78.32%\n",
      "Epoch 026: | Train Loss: 0.0736 | Train Acc: 97.59 Val. Loss: 0.7372 | Val. Acc: 79.38%\n",
      "Epoch 027: | Train Loss: 0.0648 | Train Acc: 97.72 Val. Loss: 0.7841 | Val. Acc: 76.90%\n",
      "Epoch 028: | Train Loss: 0.0612 | Train Acc: 97.93 Val. Loss: 0.8139 | Val. Acc: 76.42%\n",
      "Epoch 029: | Train Loss: 0.0638 | Train Acc: 97.57 Val. Loss: 0.6983 | Val. Acc: 79.86%\n",
      "Epoch 030: | Train Loss: 0.0651 | Train Acc: 97.72 Val. Loss: 0.8307 | Val. Acc: 78.32%\n",
      "Epoch 031: | Train Loss: 0.0550 | Train Acc: 98.25 Val. Loss: 0.7315 | Val. Acc: 79.86%\n",
      "Epoch 032: | Train Loss: 0.0654 | Train Acc: 97.66 Val. Loss: 0.7444 | Val. Acc: 79.38%\n",
      "Epoch 033: | Train Loss: 0.0584 | Train Acc: 97.85 Val. Loss: 0.7663 | Val. Acc: 78.79%\n",
      "Epoch 034: | Train Loss: 0.0710 | Train Acc: 97.45 Val. Loss: 0.8056 | Val. Acc: 78.55%\n",
      "Epoch 035: | Train Loss: 0.0616 | Train Acc: 97.82 Val. Loss: 0.7962 | Val. Acc: 79.15%\n",
      "Epoch 036: | Train Loss: 0.0642 | Train Acc: 97.65 Val. Loss: 0.8560 | Val. Acc: 79.74%\n",
      "Epoch 037: | Train Loss: 0.0630 | Train Acc: 97.81 Val. Loss: 0.7679 | Val. Acc: 80.09%\n",
      "Epoch 038: | Train Loss: 0.0538 | Train Acc: 98.19 Val. Loss: 0.8400 | Val. Acc: 79.15%\n",
      "Epoch 039: | Train Loss: 0.0510 | Train Acc: 98.24 Val. Loss: 0.7665 | Val. Acc: 80.45%\n",
      "Epoch 040: | Train Loss: 0.0538 | Train Acc: 98.06 Val. Loss: 0.7866 | Val. Acc: 78.08%\n",
      "Epoch 041: | Train Loss: 0.0503 | Train Acc: 98.19 Val. Loss: 0.7996 | Val. Acc: 80.09%\n",
      "Epoch 042: | Train Loss: 0.0495 | Train Acc: 98.28 Val. Loss: 0.8575 | Val. Acc: 78.44%\n",
      "Epoch 043: | Train Loss: 0.0435 | Train Acc: 98.55 Val. Loss: 0.8524 | Val. Acc: 79.15%\n",
      "Epoch 044: | Train Loss: 0.0524 | Train Acc: 98.28 Val. Loss: 0.8379 | Val. Acc: 79.74%\n",
      "Epoch 045: | Train Loss: 0.0421 | Train Acc: 98.31 Val. Loss: 0.8654 | Val. Acc: 79.27%\n",
      "Epoch 046: | Train Loss: 0.0528 | Train Acc: 98.07 Val. Loss: 0.8094 | Val. Acc: 80.81%\n",
      "Epoch 047: | Train Loss: 0.0501 | Train Acc: 98.42 Val. Loss: 0.7907 | Val. Acc: 79.74%\n",
      "Epoch 048: | Train Loss: 0.0445 | Train Acc: 98.49 Val. Loss: 0.8470 | Val. Acc: 79.74%\n",
      "Epoch 049: | Train Loss: 0.0462 | Train Acc: 98.27 Val. Loss: 0.8190 | Val. Acc: 79.86%\n",
      "Epoch 050: | Train Loss: 0.0479 | Train Acc: 98.42 Val. Loss: 0.9199 | Val. Acc: 79.74%\n",
      "Epoch 051: | Train Loss: 0.0401 | Train Acc: 98.77 Val. Loss: 0.8598 | Val. Acc: 79.62%\n",
      "Epoch 052: | Train Loss: 0.0441 | Train Acc: 98.68 Val. Loss: 0.8595 | Val. Acc: 78.32%\n",
      "Epoch 053: | Train Loss: 0.0395 | Train Acc: 98.61 Val. Loss: 0.8248 | Val. Acc: 80.57%\n",
      "Epoch 054: | Train Loss: 0.0430 | Train Acc: 98.52 Val. Loss: 0.8365 | Val. Acc: 82.35%\n",
      "Epoch 055: | Train Loss: 0.0435 | Train Acc: 98.47 Val. Loss: 0.9302 | Val. Acc: 79.50%\n",
      "Epoch 056: | Train Loss: 0.0431 | Train Acc: 98.47 Val. Loss: 0.9269 | Val. Acc: 80.69%\n",
      "Epoch 057: | Train Loss: 0.0445 | Train Acc: 98.37 Val. Loss: 0.8532 | Val. Acc: 79.62%\n",
      "Epoch 058: | Train Loss: 0.0419 | Train Acc: 98.40 Val. Loss: 0.8245 | Val. Acc: 79.74%\n",
      "Epoch 059: | Train Loss: 0.0493 | Train Acc: 98.19 Val. Loss: 0.8328 | Val. Acc: 81.16%\n",
      "Epoch 060: | Train Loss: 0.0465 | Train Acc: 98.46 Val. Loss: 0.9317 | Val. Acc: 79.74%\n",
      "Epoch 061: | Train Loss: 0.0370 | Train Acc: 98.74 Val. Loss: 0.7858 | Val. Acc: 80.57%\n",
      "Epoch 062: | Train Loss: 0.0386 | Train Acc: 98.53 Val. Loss: 0.8951 | Val. Acc: 78.67%\n",
      "Epoch 063: | Train Loss: 0.0337 | Train Acc: 98.80 Val. Loss: 0.8060 | Val. Acc: 79.50%\n",
      "Epoch 064: | Train Loss: 0.0390 | Train Acc: 98.55 Val. Loss: 0.9057 | Val. Acc: 78.67%\n",
      "Epoch 065: | Train Loss: 0.0403 | Train Acc: 98.53 Val. Loss: 0.8862 | Val. Acc: 80.92%\n",
      "Epoch 066: | Train Loss: 0.0357 | Train Acc: 98.84 Val. Loss: 0.9028 | Val. Acc: 79.15%\n",
      "Epoch 067: | Train Loss: 0.0358 | Train Acc: 98.68 Val. Loss: 0.8849 | Val. Acc: 78.79%\n",
      "Epoch 068: | Train Loss: 0.0399 | Train Acc: 98.79 Val. Loss: 0.8781 | Val. Acc: 79.38%\n",
      "Epoch 069: | Train Loss: 0.0394 | Train Acc: 98.68 Val. Loss: 0.9556 | Val. Acc: 80.57%\n",
      "Epoch 070: | Train Loss: 0.0314 | Train Acc: 98.99 Val. Loss: 0.8526 | Val. Acc: 80.57%\n",
      "Epoch 071: | Train Loss: 0.0276 | Train Acc: 99.16 Val. Loss: 0.8971 | Val. Acc: 80.33%\n",
      "Epoch 072: | Train Loss: 0.0337 | Train Acc: 98.62 Val. Loss: 0.8655 | Val. Acc: 79.98%\n",
      "Epoch 073: | Train Loss: 0.0347 | Train Acc: 98.71 Val. Loss: 0.9494 | Val. Acc: 79.15%\n",
      "Epoch 074: | Train Loss: 0.0297 | Train Acc: 98.90 Val. Loss: 1.0096 | Val. Acc: 79.98%\n",
      "Epoch 075: | Train Loss: 0.0344 | Train Acc: 98.73 Val. Loss: 0.9888 | Val. Acc: 78.67%\n",
      "Epoch 076: | Train Loss: 0.0359 | Train Acc: 98.89 Val. Loss: 0.9615 | Val. Acc: 79.03%\n",
      "Epoch 077: | Train Loss: 0.0295 | Train Acc: 98.96 Val. Loss: 0.8650 | Val. Acc: 80.81%\n",
      "Epoch 078: | Train Loss: 0.0343 | Train Acc: 98.68 Val. Loss: 0.9467 | Val. Acc: 79.86%\n",
      "Epoch 079: | Train Loss: 0.0311 | Train Acc: 98.80 Val. Loss: 0.9194 | Val. Acc: 79.03%\n",
      "Epoch 080: | Train Loss: 0.0273 | Train Acc: 99.13 Val. Loss: 1.0220 | Val. Acc: 78.55%\n",
      "Epoch 081: | Train Loss: 0.0361 | Train Acc: 98.76 Val. Loss: 0.9661 | Val. Acc: 79.98%\n",
      "Epoch 082: | Train Loss: 0.0300 | Train Acc: 98.89 Val. Loss: 0.9992 | Val. Acc: 79.86%\n",
      "Epoch 083: | Train Loss: 0.0358 | Train Acc: 98.61 Val. Loss: 0.9142 | Val. Acc: 79.15%\n",
      "Epoch 084: | Train Loss: 0.0308 | Train Acc: 99.02 Val. Loss: 0.9716 | Val. Acc: 78.67%\n",
      "Epoch 085: | Train Loss: 0.0400 | Train Acc: 98.46 Val. Loss: 0.9675 | Val. Acc: 79.62%\n",
      "Epoch 086: | Train Loss: 0.0344 | Train Acc: 98.83 Val. Loss: 0.9965 | Val. Acc: 79.50%\n",
      "Epoch 087: | Train Loss: 0.0246 | Train Acc: 99.24 Val. Loss: 1.0002 | Val. Acc: 78.79%\n",
      "Epoch 088: | Train Loss: 0.0306 | Train Acc: 98.86 Val. Loss: 1.0341 | Val. Acc: 79.38%\n",
      "Epoch 089: | Train Loss: 0.0309 | Train Acc: 98.95 Val. Loss: 1.0093 | Val. Acc: 79.15%\n",
      "Epoch 090: | Train Loss: 0.0249 | Train Acc: 99.02 Val. Loss: 0.9447 | Val. Acc: 80.92%\n",
      "Epoch 091: | Train Loss: 0.0265 | Train Acc: 99.19 Val. Loss: 0.9572 | Val. Acc: 79.03%\n",
      "Epoch 092: | Train Loss: 0.0277 | Train Acc: 99.04 Val. Loss: 1.0034 | Val. Acc: 79.86%\n",
      "Epoch 093: | Train Loss: 0.0327 | Train Acc: 98.87 Val. Loss: 1.0003 | Val. Acc: 79.86%\n",
      "Epoch 094: | Train Loss: 0.0327 | Train Acc: 98.80 Val. Loss: 0.9999 | Val. Acc: 79.27%\n",
      "Epoch 095: | Train Loss: 0.0298 | Train Acc: 98.95 Val. Loss: 1.0014 | Val. Acc: 78.08%\n",
      "Epoch 096: | Train Loss: 0.0301 | Train Acc: 98.99 Val. Loss: 1.0488 | Val. Acc: 79.86%\n",
      "Epoch 097: | Train Loss: 0.0331 | Train Acc: 98.89 Val. Loss: 0.9648 | Val. Acc: 77.61%\n",
      "Epoch 098: | Train Loss: 0.0278 | Train Acc: 98.87 Val. Loss: 1.1533 | Val. Acc: 77.01%\n",
      "Epoch 099: | Train Loss: 0.0286 | Train Acc: 98.93 Val. Loss: 1.0591 | Val. Acc: 79.50%\n",
      "Epoch 100: | Train Loss: 0.0288 | Train Acc: 98.98 Val. Loss: 1.0888 | Val. Acc: 78.55%\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, device, train_loader, criterion, optimizer)\n",
    "    valid_loss, valid_acc = evaluate(model, device, valid_loader, criterion)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "    print(f'Epoch {epoch+1:03d}: | Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f} Val. Loss: {valid_loss:.4f} | Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4129 | Test Acc: 81.07%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, device, test_loader, criterion, checkpoint='model.pt')\n",
    "print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ayn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4698125b34f9b3056e7b596654ef06bea4fe54a8b707ab96252cf01711dc60f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
